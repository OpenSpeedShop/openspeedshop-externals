Open|SpeedShop (O|SS)
Build and Installation Guide
Version 2.3.0/2.3.1
January 18, 2017


Introduction	3
What is CBTF and how does it relate to O|SS?	3
A new O|SS build method: SPACK	3
Some Initial Notes for the install-tool build method	4
Prerequisite Packages	6
Ubuntu/Debian:	6
RedHat/Fedora:	7
SLES/SUSE:	7
Building from the Release Tarballs (located on https://www.openspeedshop.org/downloads)	9
Installation Information	10
Building the CUDA centric new O|SS GUI	17
Module Files, Dotkits, and softenv files	18
Install tool example commands from various systems	19
Generic Laptop or Desktop Platform Installation Examples	19
Build only the krell-root	19
Build cbtf components using the krell-root	19
Build only OSS using the cbtf components and the krell-root	19
Build everything: the krell-root, the cbtf components, and OSS using cbtf instrumentor	19
Cray Platform Install Examples	21
Instructions for building O|SS CBTF based versions on Cray platforms	21
Building for the compute nodes	21
Setup up the build environment for building for the compute nodes	21
Build the compute node version of krellroot	22
Build the compute node versions of O|SS for the CBTF version	23
Building for the front-end or login nodes	24
Setup up the build environment for building for the front-end or login nodes	24
Build the front-end node version of krellroot	24
Build the front-end node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl	24
Build the front-end node version of O|SS for the CBTF version	25
Blue Gene Systems (only offline supported) Install Examples	25
Compute node -- O|SS collectors and runtimes	25
Build O|SS collectors and runtimes without using krell root components	25
Build krell root components for compute node	26
Build O|SS collectors and runtimes using krell root components	26
Front end node -- O|SS viewer	26
Build O|SS Viewer not using krell root components	26
Build krell root components	26
Build O|SS viewer using krell root components	26
Intel MIC (KNL) Platform Install Examples	26
Intel MIC (KNC) Co-Processor Platform Install Examples	26
Instructions for building O|SS CBTF based versions on Intel MIC platforms	27
Building for the compute nodes	27
Setup up the build environment for building for the compute nodes	27
Build the compute node version of krellroot	27
Build the compute node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl	27
Build the compute node versions of O|SS for the CBTF version	27
Building for the front-end or login nodes	28
Setup up the build environment for building for the front-end or login nodes	28
Build the front-end node version of krellroot	28
Build the front-end node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl	28
Build the front-end node version of O|SS for the CBTF version	28
ARM Platform Install Examples	29
Instructions for O|SS CBTF based versions on ARM platforms	29
Building for the ARM platform	29
Setup up the build environment for building for the front-end or login nodes	29
Build the krellroot - components needed to support building cbtf and O|SS	29
Build cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl	29
Build O|SS for the CBTF version	30
Power 8 Platform Install Examples	30
Instructions for O|SS CBTF based versions on Power 8 platforms	30
Building for the POWER8 platform	30
Setup up the build environment for building for the front-end or login nodes	30
Build the krellroot - components needed to support building cbtf and O|SS	30
Build cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl	30
Build O|SS for the CBTF version	31
To Run O|SS	32
Environment Setup	32
OPENSS_PLUGIN_PATH	32
LD_LIBRARY_PATH	32
DYNINSTAPI_RT_LIB	32
OPENSS_MPI_IMPLEMENTATION (for offline mode of operation)	32
PATH	33
OPENSS_RAWDATA_DIR (offline mode of operation only)	33
OPENSS_DB_DIR	33
Runtime Environment Examples	33
Generic Cluster Module File Example	34
O|SS module file example	34
Generic Cluster Dotkit File Example	35
O|SS dotkit example	35
Blue Gene/Q Softenv File Example	36
Offline version (no krell root usage)	36
Offline version (with krell root usage)	36
Cray Platform Module File Example	37
Cray platform CBTF version module file example	37
Intel MIC Platform Module File Examples	38
Intel MIC (KNL) platform CBTF version module file example	38
Intel MIC (KNC co-processor) platform CBTF version module file example	38
ARM platform CBTF version module file example	39
Power 8 platform CBTF version module file example	39
How to use SPACK to build O|SS	39
Spack creates module files for O|SS and other packages	40



Introduction 

This document supports the Open|SpeedShop (O|SS) 2.3 release and subsequent updates.    

O|SS release 2.3 comes with an install script (install-tool) that will build all the supporting components (also referred to as the krellroot) and O|SS itself, in one step.  Or the builder can invoke the script once to build the supporting component set (krellroot) and another time to build O|SS.   

The install script relies on arguments to the script instead of environment variables like the previous install script (install.sh).  With install-tool, the environment variables are set internally to the script so that the builder is not required to keep track of the numerous build environment variables that O|SS uses.  In addition, more build environment variables are set by default, requiring less builder-required action.  

The new install script also supports building the Component Based Tool Framework (CBTF) for the new O|SS CBTF instrumentor version that is described below.

What is CBTF and how does it relate to O|SS?

CBTF stands for Component Based Tool Framework and is a key scalability feature for the new version of O|SS that uses CBTF as its instrumentation methodology.  The O|SS team has completed the base functionality for the process of changing the mechanisms that gather the performance data and transfer the performance data from the application, where it was gathered, to the O|SS client where it is stored into an SQLite database file.  In order for O|SS to operate efficiently at high rank and thread counts it could not continue to write raw data files to disk and then read them up at the end of the application execution.  This is a bottleneck, which is too time-consuming at high processor counts.  So, CBTF version of O|SS using the new CBTF gathering and transfer mechanisms which transmit raw performance data to the client tool without writing files.   This will be significantly faster than the offline version.   CBTF can also be used independently from O|SS to rapidly prototype tools.    Please see the CBTF wiki at:    https://github.com/OpenSpeedShop/cbtf/wiki for more information.

A new O|SS build method: SPACK

General information about the spack build/package manager can be found at: https://github.com/LLNL/spack/wiki.  Spack is a flexible package manager for HPC. It is intended to let you build for many combinations of compiler, architectures, dependency libraries, and build configurations, all with a friendly, intuitive user interface. Spack draws some ideas from Homebrew and Nix, with its own pure Python package format, extensive internal support for managing software dependency graphs, and powerful command line syntax.
The O|SS team has been working on creating spack package files for building O|SS under the spack build system.    The O|SS build under spack is generally working for builds on clusters and pc/laptop type systems.   Cray support is not ready yet.  

Once spack is downloaded and set up, to build O|SS can be as easy as this command:
          spack install openspeedshop

See the spack sections below for more specific information.

Some Initial Notes for the install-tool build method

* In addition to some of the open source components using the “cmake” build tools to build their components, the O|SS team is now using cmake as the build tool for O|SS, and the CBTF components that the team supplies.    So, cmake is now a build pre-requisite package.   Versions of cmake with a version number of 3.0 and above are favored.   cmake version 2.12.2 is a minimum.
* Note: For version 2.2 and 2.3, please replace “--build-cbtf” with “--build-cbtf-all”, as we are now using cmake as our underlying build tool instead of GNU autotools.  With cmake we are building cbtf, cbtf-krell, cbtf-argonavis, and cbtf-lanl separately.  When the GNU autotools were used we built all of the CBTF components within one build.   Now the CBTF components are individually built (four separate builds) when “--build-cbtf-all” is used. 
* Caution: boost-1.60 to 1.62 versions cause compile errors when building the cbtf components and the new cuda GUI.  These are known boost problems.   Please use a different version of boost when building O|SS.
* Caution: When installing a new version, please install into an empty/clean directory, as we do not clean the install directories out upon install-tool invocation.   You have end up with multiple copies of the same component, which can lead to execution issues.
* We refer to root or krell-root packages or building the krell-root.   What does this mean?   These are the external packages that O|SS and/or CBTF uses as part of the tool.   Sometimes one or several of these external packages are not available on the platform that O|SS and/or CBTF is being installed on, so we have provided the ability to build and install them.   The root/external packages are provided as a convenience.  You could install these packages without the use of the krell-root on the install system, if desired.
* The CBTF version of O|SS is now the default and official version of O|SS, as of this 2.3 release.   The offline version is now legacy, but the offline capabilities have been built into the CBTF version and is usable on most platforms (not Blue Gene systems).
* You can install O|SS in non-standard locations (root permission is not needed)
* Versions of O|SS 2.3 and beyond can be installed using the information in this document.
* NOTE: O|SS is normally built with the GNU compilers, but now building with the Intel compiler is supported.  Options to the install-tool have been created to allow building with either the gnu or intel compilers.   Using the gnu compiler is the default.
o --build-compiler gnu
o --build-compiler intel  
* O|SS cannot be built with PGI, Cray, or other compilers.   Even though O|SS only builds with GNU or Intel compilers, O|SS supports performance analysis of applications built with a wide variety of compilers, including GNU, Intel, PGI, Pathscale, and Cray.
* There are a number of development environment packages that are required if you are starting from a clean install on your desktop or laptop.   See the Prerequisite Packages section for the list of packages needed on non-development systems.  Most laboratory systems normally have all the required packages installed, because most tools need these packages to build and execute successfully.
* There is now an option to build the krell root components that might be missing on the system or needed separately from O|SS.  The builder can build the krell root then use that installation to build CBTF and/or O|SS.   Or, you can choose to build all components into the same install directory.
* The install-tool script supports building both O|SS modes of execution:
o cbtf -- Performance information is transported from the application across a TCP/IP network as it is collected and stored in the O|SS database on the fly.
o NOW LEGACY: offline -- Write raw performance information to shared file system disk files while the application is executing.  Then convert the raw data files to an O|SS database file when the application completes execution.
* install-tool script options for building O|SS are listed and described below:
o To build the new CBTF version use three invocations of the install-tool script individually, in the order listed.  
* "--build-krell-root"
* "--build-cbtf-all"
* "--build-oss"
o NOW LEGACY: To build the offline version use two invocations of the install-tool script individually, in the order listed.
* "--build-krell-root"
* "--build-offline"

* The examples below are for illustration and will need adjustment based on the software installations on your particular system.
* On systems with heterogeneous front-end and compute node processor sets, multiple install-tool invocations are required.  Examples of these types of systems are the BG/Q, some Cray systems, or on any other systems where targeted builds are needed.  We suggest a separate set of builds, one set for the front-end viewer tool build and another set of builds for the compute node runtimes and performance data collectors build.   
o For building the collectors and runtimes that execute on the compute node:
* "--build-krell-root" using --target-arch=<platform>
* "--build-cbtf-all" using --target-arch=<platform>
o NOW LEGACY: For building the client tool that runs on the front-end node:
* "--build-krell-root"
* "--build-offline"

o Note: It appears that some systems with heterogeneous front-end versus compute node processor types default to the compute node compilers.  Most of our instructions assume the default compilers are front-end compilers.  Putting "CXX=g++" and "CC=gcc" in front of the install-tool command when building the front-end version of O|SS may get around this issue on platforms where the defaults are not what we expected (defaults to compute node compiler version).


Prerequisite Packages

There are some prerequisite packages that are necessary for building and running the O|SS performance tool.   Most will be present on a system that is used for debugging and performance analysis.  This is true of most national laboratory clusters, Cray, and Blue Gene platforms.   However, if you are installing on machines without the necessary tool supporting packages installed then this section might be helpful.

Ubuntu/Debian:

It is necessary to apply these packages to the base system installation, if they are not already installed, in order to successfully build O|SS and the external tool specific packages required by O|SS.  Example required packages based on Ubuntu 16.04 install.

Packages to aid in development:
sudo apt-get install git cmake cvs
Packages to aid in downloading and building:
sudo apt-get install flex bison libxml2-dev python-dev g++ make patch environment-modules libz-dev binutils-dev libdwarf-dev libelf-dev
Packages that install-tool will build if you don't have them installed: (but makes build shorter if these are installed)
sudo apt-get install automake autoconf libtool m4 libltdl-dev(--build-autotools)
sudo apt-get install binutils binutils-dev (--build-binutils) 
sudo apt-get install libelf1 elfutils libelf-dev (--build-libelf)
sudo apt-get install sqlite3 (--build-sqlite)
If the package: qt3-apps-dev is available use it, if not, you may need these (libx11-dev libxext-dev) instead.


RedHat/Fedora:

It is necessary to apply a list of supporting packages to the base system installation, if they are not already installed, in order to successfully build O|SS and the external tool specific packages required by O|SS.  This list is somewhat variable depending on what was installed during the initial installation and prior to attempting to install O|SS.  Previous experiences have resulted in this list of candidate packages:

         yum install -y rpm-build \
               gcc gcc-c++ \
               openmpi \
               patch \
               autoconf automake \
               elfutils-libelf elfutils-libelf-devel \
               libxml2 libxml2-devel \
               binutils binutils-devel \
               python python-devel \
               flex bison bison-devel bison-runtime \
               libtool libtool-ltdl libtool-ltdl-devel cmake git

SLES/SUSE:

It is necessary to apply these packages to the base system installation, if they are not already installed, in order to successfully build O|SS and the external tool specific packages required by O|SS.

Packages to aid in development:
Modules git 
Packages to aid in downloading and building:
flex bison rpm libxml2 libxml2-dev python-dev g++ make patch cmake
Packages that install-tool will build if you don't have them installed: (but makes the build shorter if these are installed)
qt3 qt3-devel


If your system was installed with any development package group, then the need for extra package installs may be reduced significantly.

Using the install-tool that comes with the O|SS release will install many of the key open source tool related components, but the above mentioned system components are required to be installed by the system administrator.





Building from the Release Tarballs (located on https://www.openspeedshop.org/downloads)

The release top-level directory contains the install script (install-tool) that is intended to make the build and installation of the external tool support components (known as the krell root) that are needed to support O|SS and O|SS itself easier.

Because O|SS depends on components that are highly dependent on operating system interfaces and libraries, it is difficult to produce executable rpms for every installation platform.  This is the reason why we offer the source build installation instead of rpms.



Installation Information

Please gunzip and untar the openspeedshop-release-2.3.0 tar.gz file and change directory into the openspeedshop-release-2.3 directory.  

For example:
tar -xzvf openspeedshop-release-2.3.0.tar.gz
cd openspeedshop-release-2.3

Inside the top-level release directory is the key script, install-tool that can be used in building the O|SS performance tool.

The script, install-tool has a help option:
"install-tool --help"
That can provide information about the possible options a builder of O|SS could use.

The typical build for O|SS is done with an install line something like this:
./install-tool --build-krell-root 
       --krell-root-prefix /opt/krellroot_v2.3.0 
       --with-openmpi /opt/openmpi-1.8.2

./install-tool   --build-cbtf-all
--cbtf-prefix /opt/cbtf_v2.3.0
       --krell-root-prefix /opt/krellroot_v2.3.0 
--with-openmpi /opt/openmpi-1.8.2


./install-tool   --build-oss
--openss-prefix /opt/osscbtf_v2.3.0
       --krell-root-prefix /opt/krellroot_v2.3.0 
--with-openmpi /opt/openmpi-1.8.2

The first install line above builds all the external (krell root) packages that O|SS needs and installs them in /opt/krellroot_v2.3.0.  The second install line above uses those external packages to build the Component Based Tool Framework (CBTF).  It also uses the openmpi MPI implementation to build the O|SS MPI experiment collectors.  The third install line uses both the external packages built by the krell root install line and the cbtf package built by the cbtf-all install line to build the O|SS client tools.

Hint:
To get the paths for the "--with-mpt" or other mpi or other "--with" option clauses is to:
  1) module load mpi-sgi/mpt.2.12r26
  2) echo $LD_LIBRARY_PATH
  3) Look for the MPT path (up to the "/lib" and use that in the "--with-mpt" clause in the install-tool command.

Example:  echo $LD_LIBRARY_PATH
/nasa/sgi/mpt/2.12r26/lib:/home4/jgalarow/python_root/lib

Use this --with-mpt clause: "--with-mpt /nasa/sgi/mpt/2.12r26"

However, on some platforms the typical build install line is not adequate.  Platforms like the Cray and Blue Gene requires more options to the install-tool script.   If you are familiar with the previous install.sh script method of building O|SS, in that build scenario extra environment variables needed to be set.  With install-tool, we need additional arguments to the script to specify things like the target platform, so there is a --target-arch <target> option available in the install-tool script.   For example, to build on a Cray system, this is an example build line:
./install-tool   --build-krell-root --target-arch cray
--krell-root-prefix /tmp/work/<userid>/krellroot_v2.3.0 
--with-boost <boost install directory> 
--with-mpich2 /opt/cray/mpt/default/gni/mpich2-gnu/47

./install-tool   --build-cbtf-all --target-arch cray
--cbtf-prefix /tmp/work/<userid>/cbtf_v2.3.0
       --krell-root-prefix /tmp/work/<userid>/krellroot_v2.3.0 
--with-mpich2 /opt/cray/mpt/default/gni/mpich2-gnu/47


./install-tool   --build-oss --target-arch cray
--openss-prefix /tmp/work/<userid>/osscbtf_v2.3.0 
--krell-root-prefix /tmp/work/<userid>/krellroot_v2.3.0 
--with-boost <boost install directory> 
--with-mpich /opt/cray/mpt/default/gni/mpich2-gnu/47

One can specify their own versions of the components needed by O|SS to build properly or you can rely on the defaults that are installed on the system and used automatically by the build script.   If the default system installed component is determined to be not adequate for the O|SS build (missing development headers) then the install script will build a version of that component into the externals/root directory.   If the builder has a specific version of a component, such as, PAPI, they may use the "--with-papi" option to specify the path to that installation and O|SS will be built using that installation of PAPI.

One can also build individual components and specify the location to place the component.   Here is the "install-tool --help" output:
$
$ ./install-tool --help 
usage: ./install-tool [option] 

--help, -h 
  This help text. 

Introduction: 
  This install script can be used to build the krell externals package (--build-krell-root), 
  the CBTF components and libraries (--build-cbtf-all), and/or the default version of OpenSpeedShop 
  which now contains both the offline mode of operation and the cbtf mode of operation with the: (--build-oss) install-tool option. 

  Typical usages examples are followed by the option descriptions. More examples and explanations 
  can be found in the Build and Install Guides for CBTF and O|SS. 

Typical usage example for cluster/PC build: 
  # Build only the krell-root 
  ./install-tool --build-krell-root 
                 --krell-root-prefix /opt/krellroot_v2.3.0 
                 --with-openmpi /opt/openmpi-1.8.2 

  # Build cbtf components using the krell-root 
  ./install-tool --build-cbtf-all 
                 --cbtf-prefix /opt/cbtf_v2.3.0 
                 --krell-root-prefix /opt/krellroot_v2.3.0 
                 --with-openmpi /opt/openmpi-1.8.2 
                 --with-cupti /usr/local/cuda-7.0/extras/CUPTI 
                 --with-cuda /usr/local/cuda-7.0 

  # Build only OSS using the cbtf components and the krell-root 
  ./install-tool --build-oss 
                 --cbtf-prefix /opt/cbtf_v2.3.0 
                 --krell-root-prefix /opt/krellroot_v2.3.0 
                 --openss-prefix /opt/osscbtf_v2.3.0 
                 --with-openmpi /opt/openmpi-1.8.2 
                 --with-cupti /usr/local/cuda-7.0/extras/CUPTI 
                 --with-cuda /usr/local/cuda-7.0 

Typical in one command usage example for cluster/PC build: 

  # Build the krell-root, the cbtf components, and OSS using cbtf instrumentor 
  ./install-tool --build-all 
                 --cbtf-prefix /opt/cbtf_v2.3.0 
                 --krell-root-prefix /opt/krellroot_v2.3.0 
                 --openss-prefix /opt/osscbtf_v2.3.0 
                 --with-openmpi /opt/openmpi-1.8.2 

Option Descriptions: 

--krell-root-prefix <directory> 
  Where <directory> is the location to install the components 
  needed for building CBTF and OpenSpeedShop 
  and its supporting tools and libraries. The default 
  is /opt/KRELLROOT. It is not recommended to use /usr. 
  NOTE: This option will override any setting for 
  the environment variable KRELL_ROOT_PREFIX. 

  NOTE: This prefix should be used as the install path 
        when building a specific component (for example: 
        install-tool --build-libelf --krell-root-prefix /opt/libelf-0.8.13 

--cbtf-prefix <directory> 
  Where <directory> is the location to install CBTF 
  and its supporting tools and libraries. The default 
  is /opt/CBTF. It is not recommended to use /usr. 
  NOTE: This option will override any setting for 
  the environment variable CBTF_PREFIX. 

--openss-prefix <directory> 
  Where <directory> is the location to install OpenSpeedShop 
  and its supporting tools and libraries. The default 
  is /opt/OSS. It is not recommended to use /usr. 
  NOTE: This option will override any setting for 
  the environment variable OPENSS_PREFIX. 

--build-all 
  Build the krell-root, cbtf components, and openspeedshop 
  The cbtf-prefix, krell-root-prefix, and the openss-prefix must be specified 

--build-cbtf-all 
  Build only the cbtf components using the existing krell-root 
  The krell-root-prefix and cbtf-install-prefix must be specified 

--build-krell-root 
  Build only the krell-root. The krell-root-prefix must be specified 

--build-oss ) 
  Build only the OpenSpeedShop client component for the cbtf instrumentor 
  using the cbtf and krell-root components. 

  Specify a target architecture 
--target-arch <target> 
  Where acceptable target values are: cray, bgp, bgq, bgqfe, arm, power8

  Specify the compiler to be used for the build, gnu is default 
--build-compiler {gnu, intel} 

  Use these MPI installations when building 
--with-mpich <directory> 
--with-mpich2  <directory> 
--with-mpich2-driver  <driver name> 
--with-mvapich  <directory> 
--with-mvapich2  <directory> 
--with-mvapich2-driver  <driver name> 
--with-openmpi  <directory> 
--with-mpt  <directory> 
  Where <directory> is the install path to the mpi implementation 
  to use for MPI support. 

  Build only the component specified by the build clause. 
  The krell-root-prefix must be specified and components will 
  be installed into that krell-root-prefix specified directory path. 
--build-autotools 
--build-bison 
--build-boost 
--build-xercesc 
--build-binutils 
--build-bison 
--build-cbtfargonavisgui 
--build-dyninst 
--build-libelf 
--build-libdwarf 
--build-libmonitor 
--build-libunwind 
--build-mrnet 
--build-ompt 
--build-papi 
--build-python 
--build-sqlite 
--build-symtabapi 
--build-cmake 
--build-ptgf 
--build-ptgfossgui 
--build-qcustomplot 
--build-qgraphviz 
--build-serene 
--build-qt3 
--build-vampirtrace 

  Enable certain configuration options 
--enable-bfd-symbol-resolution|--enable-bfd 
--enable-debug 

  Force these components to be built and installed into the krellroot 
  or OSS install directories. 
--force-binutils-build 
--force-boost-build 
--force-dyninst-build 
--force-libelf-build 
--force-libdwarf-build 
--force-libunwind-build 
--force-papi-build 
--force-qt3-build 
--force-sqlite-build 
--force-xercesc-build 
--force-ompt-build 
  Build all of the above by force 
--force-all 

--skip-binutils-build 
--skip-boost-build 
--skip-dyninst-build 
--skip-libdwarf-build 
--skip-libelf-build 
--skip-libunwind-build 
--skip-mrnet-build 
--skip-papi-build 
--skip-symtabapi-build 
--skip-vampirtrace-build 
--skip-qt3-build 
--skip-ompt-build 


  Use these non-root or alternative components when building 
--with-binutils <directory> 
--with-boost <directory> 
--with-dyninst <directory> 
--with-expat <directory> 
--with-libelf <directory> 
--with-libdwarf <directory> 
--with-libmonitor <directory> 
--with-libunwind <directory> 
--with-mrnet <directory> 
--with-papi <directory> 
--with-python <directory> 
--with-python-vers <version number> 
--with-qt3 <directory> 
--with-sqlite <directory> 
--with-symtabapi <directory> 
--with-xercesc <directory> 
--with-otf <directory> 
--with-vt <directory> 
  Where <directory> is the install path to the alternative component. 

  Use these non-root or alternative compute node components when building a cbtf-krell fe 
  that points to targeted runtimes (cray, mic platforms) 
--with-cn-cbtf <directory> 
--with-cn-cbtf-krell <directory> 
--with-cn-binutils <directory> 
--with-cn-dyninst <directory> 
--with-cn-libelf <directory> 
--with-cn-libdwarf <directory> 
--with-cn-libmonitor <directory> 
--with-cn-libunwind <directory> 
--with-cn-mrnet <directory> 
--with-cn-symtabapi <directory> 
--with-cn-xercesc <directory> 
--with-cn-papi <directory> 
--with-cn-boost <directory> 
  Where <directory> is the install path to the alternative component. 

--with-tls < explicit | implicit > 
  where the default is implicit 

  Only build ptgf gui related components, not qt3 related 
--use-only-ptgf

Optional install-tool arguments are needed for configuring O|SS for MPI experiments.   If the installation of O|SS is intended to support running MPI specific O|SS experiments, then one or more MPI implementation arguments must be specified.   The O|SS install-tool script will build MPI experiment collectors for one or more of these MPI implementations by specifying one or more of the "--with-<mpi implementation>" arguments:

For openMPI: 
 --with-openmpi <openmpi install path> 
For mpich:
--with-mpich <mpich install path> 
For mpich2:
--with-mpich2 <mpich2 install path> 
For SGI mpt1:
--with-mpt <mpt install path> 
For mvapich
--with-mvapich <mvapich install path> 
For mvapich2:
--with-mvapich2 <mvapich2 install path> 


If none of the above arguments are not specified, every non-MPI specific O|SS experiments will be built and execute properly, but the mpi, mpit, mpiotf experiments will not be built.  The MPI implementation arguments are not necessary to run pcsamp, usertime, hwc, io, or fpe and variants of those experiments, even on MPI applications.   They are only needed for mpi, mpip, and mpit experiment creation because the tool needs to know the MPI data structure definitions to process MPI performance data.

The install-tool script and the O|SS configuration code will operate on those MPI arguments and will configure O|SS to recognize these MPI implementations.  When the MPI specific argument collectors are built and installed, users will have the ability to create MPI experiments (mpi, mpip, mpit) and gather performance data for MPI applications built with those specific MPI implementations.

Building the CUDA centric new O|SS GUI

There is a new graphical user interface being developed with the initial focus on the cuda experiment.   However, support for all experiments is planned.  To build this somewhat experimental tool use the following install-tool command line format:
       ./install-tool --build-cbtfargonavisgui
                                --with-openss <path to the O|SS client installation>
                                --with-cbtf <path to the CBTF installation>
                                --krell-root-prefix <path to the krell root externals installation>


For example:

./install-tool --build-cbtfargonavisgui 
                         --with-openss /opt/OSS/osscbtf_v2.3.0b2 
                         --with-cbtf /opt/OSS/cbtf_v2.3.0b2 
                         --krell-root-prefix /opt/OSS/krellroot_v2.3.0b2

See the O|SS reference/users guide for information on how to use the new graphical user interface.  The new GUI command is: openss-gui, while the existing GUIs command is: openss.

Module Files, Dotkits, and softenv files

On most systems, a module file, dotkit file, or softenv file must be created after O|SS is built and that file is then activated/loaded prior to running O|SS.   There are examples of each in the Runtime Environment Examples section below.



Install tool example commands from various systems

These examples show optional ways of building the default version of O|SS.  The krell root components, the cbtf components, and the O|SS client components are installed into separate install directories allowing the ability to update each individually.  One can use the "--build-all" option allows all the pieces to be built with one command where all the install locations are specified.  The "--krell-root-prefix", "--cbtf-prefix", and "--openss-prefix" options must be specified, so that the build script knows where to install the separate components.   Or you can pass the same install prefix for all of the three prefix options.

Generic Laptop or Desktop Platform Installation Examples

Please load the cmake module file or have cmake installed, as it is required for building some root components as well as O|SS and the CBTF components (named cbtf, cbtf-krell, cbtf-argonavis, and cbtf-lanl).
Build only the krell-root

./install-tool --build-krell-root 
--krell-root-prefix /opt/krellroot_v2.3.0 
--with-openmpi /opt/openmpi-1.8.2
Build cbtf components using the krell-root

./install-tool --build-cbtf-all 
--cbtf-prefix /opt/cbtf_only_v2.3.0 
--krell-root-prefix /opt/krellroot_v2.3.0 
--with-openmpi /opt/openmpi-1.8.2 
--with-cupti /usr/local/cuda-6.5/extras/CUPTI 
--with-cuda /usr/local/cuda-6.5

Build only OSS using the cbtf components and the krell-root

./install-tool --build-oss 
--cbtf-prefix /opt/cbtf_only_v2.3.0 
--krell-root-prefix /opt/krellroot_v2.3.0 
--openss-prefix /opt/osscbtf_v2.3.0 
--with-openmpi /opt/openmpi-1.8.2 
--with-cupti /usr/local/cuda-6.5/extras/CUPTI 
--with-cuda /usr/local/cuda-6.5

Build everything: the krell-root, the cbtf components, and OSS using cbtf instrumentor

./install-tool --build-all 
--cbtf-prefix /opt/cbtf_only_v2.3.0 
--krell-root-prefix /opt/krellroot_v2.3.0 
--openss-prefix /opt/osscbtf_v2.3.0 
--with-openmpi /opt/openmpi-1.8.2


Cray Platform Install Examples

Depending on what the default programming environment setting is, you must swap the PrgEnv-cray or PrgEnv-pgi environment module to PrgEnv-gnu.  If you are building for CUDA, then you will have to load the cudatoolkit module file.   The cmake tool is also required by the cbtf build and is not loaded by default on many Cray systems.  A possible module load/swap scenario prior to invoking the install-tool scripts is as follows:
module swap PrgEnv-pgi PrgEnv-gnu
module load cudatoolkit
module load cmake

Instructions for building O|SS CBTF based versions on Cray platforms

We have moved to a multi-step process for building on platforms where the front-end node and the compute nodes are running different processor sets.  By building the compute node O|SS and components with the compilers suggested for running on the compute nodes and by building the front-end node O|SS and components we are able to optimally support the Cray platform. 

The high level view is to first build for the compute nodes by building the components needed by O|SS and CBTF (krellroot), then build the CBTF components, and finally build O|SS.   After the component node versions are built, then we do a similar set of builds for the front-end components and O|SS and CBTF clients.

Building for the compute nodes 

This must be done first because we pass the O|SS and CBTF compute node installation directories as arguments to the front-end install-tool build commands.

Setup up the build environment for building for the compute nodes

Note: that this is an example set of module settings.   Newer or older versions of Cray software may require alternative module files to be loaded or unloaded.   This is an example from the DOD Cray platform Gordon.  It follows closely with most Cray type builds, although there are differences on the versions that are used on a particular system.    

Also, note: there is a new interface that appears to be replacing the ALPS package and aprun as the default way to launch applications on the Cray.   The CTI interface allows for launching application via native SLURM srun.   On systems where the CTI interface is being use, please use the install-tool --use-cti install-tool option and do not pass the –-with-alps option.

Another note: Some Cray systems do not install expat-static package which. causes the MRNet package build to fail.   In order to resolve this issue, we added the expat tarball to the external packages in the SOURCES directory.  To build this into the krellroot, use:  install-tool --build-expat --krell-root-prefix <install location>.  Then add --with-expat to the install-tool --build-krell-root command line to resolve this issue.

module unload PrgEnv-pgi PrgEnv-cray PrgEnv-intel 
module load PrgEnv-gnu 
module unload gcc; module load gcc/4.9.3 
export SYSROOT_DIR=/opt/cray/xc-sysroot/default 
module load cmake-3.2.2

BASE_IDIR=/p/home/app/PET/pkgs/openss 
TOOL_VERS="_v2.3.0.beta1" 
KROOT_IDIR=${BASE_IDIR}/krellroot${TOOL_VERS} 
CBTF_IDIR=${BASE_IDIR}/cbtf${TOOL_VERS} 
OSSCBTF_IDIR=${BASE_IDIR}/osscbtf${TOOL_VERS} 
OSSOFF_IDIR=${BASE_IDIR}/ossoff${TOOL_VERS} 
PAPI_IDIR=/opt/cray/papi/default 
MPICH_IDIR=/opt/cray/mpt/7.3.2/gni/mpich-gnu/5.1 
ALPS_IDIR=/opt/cray/alps/default

             export cc=gcc 
             export CC=gcc 
             export CXX=g++ 

Build the compute node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl
# Build root components runtime only 
./install-tool --runtime-only 
        --target-arch cray --target-shared 
        --build-krell-root --krell-root-prefix ${KROOT_IDIR}/compute 
        --with-mpich ${MPICH_IDIR} 
        --with-papi ${PAPI_IDIR} 
        --with-alps ${ALPS_IDIR} 

./install-tool --build-cbtf-all --runtime-only 
        --target-arch cray --target-shared 
         --cbtf-prefix ${CBTF_IDIR}/compute 
         --krell-root-prefix ${KROOT_IDIR}/compute 
         --with-mpich ${MPICH_IDIR} 
         --with-papi ${PAPI_IDIR}

Build the compute node version of krellroot
./install-tool --build-krell-root 
--runtime-only 
--target-arch cray 
--target-shared 
--krell-root-prefix /home/jgalaro/krellroot_stable/compute 
--with-papi /opt/cray/papi/5.3.1

./install-tool --build-cbtf-all
--runtime-only 
--target-arch cray 
--target-shared 
--cbtf-prefix /home/jgalaro/cbtf_only_stable/compute 
--krell-root-prefix /home/jgalaro/krellroot_stable/compute 
--with-mpich2 /opt/cray/mpt/7.0.1/gni/mpich2-gnu/48 
--with-papi /opt/cray/papi/5.3.1  
--with-dyninst /home/jgalaro/compute/dyninst-9.0.0_gcc

Build the compute node versions of O|SS for the CBTF version
Note:  There is nothing to build, as this version uses the CBTF compute node components for gathering the data.  Those were built in the --build-cbtf-all install-tool build command.



NOTE:

Because the default GNU compilers are too old on some Cray platforms and if we build with the default compilers some of the necessary software components will fail to build.   If we can build with the default GNU compilers, then this isn't an issue.  No copy or LD_LIBRARY_PATH setup needed.

If a newer compiler was loaded via a module file in order to build the tool, then after building everything you have to copy the libstdc++.so.6 from the compiler directory (because it is not available on the compute nodes) into the compute node install directory (krellroot/lib64, for example) or setup LD_LIBRARY_PATH to point to that location.

Here is an example of how it could be done.

> module list
Currently Loaded Modulefiles:
  1) modules/3.2.6.7    2) eswrap/1.0.8       3) cray-mpich/7.0.1   4) cmake-3.2.2           5) gcc/4.8.2

> which gcc
/opt/gcc/4.8.2/bin/gcc

> lsr /opt/gcc/4.8.2//snos/lib64/libstdc++.so.6
0 lrwxrwxrwx 1 root root 19 Aug 8 2014 /opt/gcc/4.8.2//snos/lib64/libstdc++.so.6 -> libstdc++.so.6.0.18

Building for the front-end or login nodes

Setup up the build environment for building for the front-end or login nodes
* module unload PrgEnv-pgi PrgEnv-gnu PrgEnv-cray PrgEnv-intel
* module unload craype-network-gemini craype-mc8
* module load cmake-3.2.2
* module load gcc
* export CXX=g++
* export cc=gcc

Build the front-end node version of krellroot
./install-tool --build-krell-root 
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-papi /opt/cray/papi/5.3.1 
--with-alps /opt/cray/xe-sysroot/default/usr 
--with-mpich2 /opt/cray/mpt/7.0.1/gni/mpich2-gnu/48

Build the front-end node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl
Note:  The --with-cn-<component> lines have compute node installations as their arguments.   We are pointing the front-end build to the compute node runtime libraries in order to run the properly built compute node versions of the runtime libraries.

./install-tool --build-cbtf-all 
--runtime-target-arch cray 
--cbtf-prefix /home/jgalaro/cbtf_only_stable 
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-mpich2 /opt/cray/mpt/7.0.1/gni/mpich2-gnu/48 
--with-cn-boost /home/jgalaro/krellroot_stable/compute 
--with-cn-mrnet /home/jgalaro/krellroot_stable/compute 
--with-cn-xercesc /home/jgalaro/krellroot_stable/compute 
--with-cn-libmonitor /home/jgalaro/krellroot_stable/compute 
--with-cn-libunwind /home/jgalaro/krellroot_stable/compute 
--with-cn-dyninst /home/jgalaro/compute/dyninst-9.0.0_gcc 
--with-cn-papi /opt/cray/papi/5.3.1 
--with-cn-cbtf-krell /home/jgalaro/cbtf_only_stable/compute 
--with-cn-cbtf /home/jgalaro/cbtf_only_stable/compute 
--with-binutils /home/jgalaro/krellroot_stable 
--with-boost /home/jgalaro/krellroot_stable 
--with-mrnet /home/jgalaro/krellroot_stable 
--with-xercesc /home/jgalaro/krellroot_stable 
--with-libmonitor /home/jgalaro/krellroot_stable 
--with-libunwind /home/jgalaro/krellroot_stable 
--with-dyninst /home/jgalaro/krellroot_stable 
--with-papi /opt/cray/papi/5.3.1

Build the front-end node version of O|SS for the CBTF version
./install-tool --target-arch cray 
--build-oss 
--openss-prefix /home/jgalaro/osscbtf_stable 
--with-cn-cbtf-krell /home/jgalaro/cbtf_only_stable/compute 
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-mpich2 /opt/cray/mpt/7.0.1/gni/mpich2-gnu/48 
--with-papi /opt/cray/papi/5.3.1 
--with-boost /home/jgalaro/krellroot_stable 
--with-mrnet /home/jgalaro/krellroot_stable 
--with-xercesc /home/jgalaro/krellroot_stable 
--with-libmonitor /home/jgalaro/krellroot_stable 
--with-libunwind /home/jgalaro/krellroot_stable 
--with-dyninst /home/jgalaro/krellroot_stable 
--with-libelf /home/jgalaro/krellroot_stable 
--with-libdwarf /home/jgalaro/krellroot_stable 
--with-binutils /home/jgalaro/krellroot_stable 
--cbtf-prefix /home/jgalaro/cbtf_only_stable

Blue Gene Systems (only offline supported) Install Examples

For the Blue Gene systems, it is still a two-step process to build the offline version of O|SS. First build the compute node collectors and runtimes that run on the compute nodes.  Note that the compute note installation paths have a "bgq" sub-directory appended to the viewer installation path by convention, not necessity.  The location must be different than that of the viewer installation as not to clobber the viewer installation.  The builder is responsible for keeping the two installations separate.   Next build the O|SS viewer that runs on the front-end node, giving this build the --with-runtime-dir that points to the O|SS compute node install directory.  That way the front-end knows where the compute node collectors and runtime libraries are located.

For the Blue Gene builds, you must build offline for the compute node first and then build offline for the front-end node.   You may build offline using the krellroot concept or not.   Using krellroot allows you to just build the new openspeedshop-2.3 updates when they arrive without building all the krellroot components over again.
Compute node -- O|SS collectors and runtimes
Build O|SS collectors and runtimes without using krell root components

./install-tool --build-offline 
--runtime-only --target-shared --target-arch bgq 
--openss-prefix /usr/global/tools/openspeedshop/oss-dev/bgq/ossoff_v2.3.0/compute 
--with-mpich2 /bgsys/drivers/ppcfloor/comm/gcc 
Build krell root components for compute node
./install-tool --runtime-only --target-shared --target-arch bgq --build-krell-root 
--krell-root-prefix /usr/global/tools/openspeedshop/ossdev/bgq/krellroot_v2.3.0/compute 
--with-mpich2 /bgsys/drivers/ppcfloor/comm/gcc
Build O|SS collectors and runtimes using krell root components

./install-tool --build-offline 
--runtime-only --target-shared --target-arch bgq
--openss-prefix /usr/global/tools/openspeedshop/oss-dev/bgq/ossoff_v2.3.0/compute
--krell-root-prefix /usr/global/tools/openspeedshop/ossdev/bgq/krellroot_v2.3.0/compute
 --with-mpich2 /bgsys/drivers/ppcfloor/comm/gcc
Front end node - O|SS viewer
Build O|SS Viewer not using krell root components
./install-tool --build-offline 
--openss-prefix /usr/global/tools/openspeedshop/oss-dev/bgq/ossoff_v2.3.0 
--with-mpich2 /bgsys/drivers/ppcfloor/comm/gcc  
--with-runtime-dir /usr/global/tools/openspeedshop/oss-dev/bgq/ossoff_v2.3.0/compute
Build krell root components
./install-tool --build-krell-root 
--krell-root-prefix /usr/global/tools/openspeedshop/oss-dev/bgq/krellroot_v2.3.0 
--with-mpich2 /bgsys/drivers/ppcfloor/comm/gcc
Build O|SS viewer using krell root components
./install-tool --build-offline 
--openss-prefix /usr/global/tools/openspeedshop/oss-dev/bgq/ossoff_v2.3.0 
--krell-root-prefix /usr/global/tools/openspeedshop/oss-dev/bgq/krellroot_v2.3.0 
--with-mpich2 /bgsys/drivers/ppcfloor/comm/gcc 
–-with-runtime-dir /usr/global/tools/openspeedshop/oss-dev/bgq/ossoff_v2.3.0/compute
 

Intel MIC (KNL) Platform Install Examples

The builds of O|SS, CBTF and supporting components are done on the Intel MIC KNL login node.   Use the generic cluster build examples above. 

Intel MIC (KNC) Co-Processor Platform Install Examples

The builds of O|SS, CBTF and supporting components are done on the Intel MIC co-process login node (with Intel MIC specific software: compilers, libraries).  This example build scenario is based on experiences from building NASA's maia system and the NERSC test bed MIC system named babbage.

The cmake tool is also required by the cbtf and O|SS builds and is not loaded by default on many systems.  A possible module load scenario prior to invoking the install-tool scripts is as follows:
module load cmake
Instructions for building O|SS CBTF based versions on Intel MIC platforms

We have moved to a multi-step process for building on platforms where the front-end node and the compute nodes are running different processor sets.  By building the compute node O|SS and components with the compilers suggested for running on the compute nodes and by building the front-end node O|SS and components we are able to optimally support the Intel MIC platform. 

The high level view is to first build for the compute nodes by building the components needed by O|SS and CBTF (krellroot), then build the CBTF components, and finally build O|SS.   After the component node versions are built, then we do a similar set of builds for the front-end components and O|SS and CBTF clients.

Building for the compute nodes 

This must be done first because we pass the O|SS and CBTF compute node installation directories as arguments to the front-end install-tool build commands.

Setup up the build environment for building for the compute nodes

Note that this is an example set of module settings.   Newer or older versions of Intel MIC specific software may require alternative module files to be loaded or unloaded.

* module load cmake
Build the compute node version of krellroot
./install-tool --build-krell-root 
--runtime-only 
--target-arch mic 
--krell-root-prefix /home/jgalaro/krellroot_stable/compute 
Build the compute node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl
./install-tool --build-cbtf-all
--runtime-only 
--target-arch mic 
--cbtf-prefix /home/jgalaro/cbtf_only_stable/compute 
--krell-root-prefix /home/jgalaro/krellroot_stable/compute 
Build the compute node versions of O|SS for the CBTF version

Note:  There is nothing to build, as this version uses the CBTF compute node components for gathering the data.  Those were built in the --build-cbtf-all install-tool build command.

Building for the front-end or login nodes

Setup up the build environment for building for the front-end or login nodes
* module load cmake-3.2.2
Build the front-end node version of krellroot
./install-tool --build-krell-root 
--krell-root-prefix /home/jgalaro/krellroot_stable 

Build the front-end node versions of cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl
Note:  The --with-cn-<component> lines have compute node installations as their arguments.   We are pointing the front-end build to the compute node runtime libraries in order to run the properly built compute node versions of the runtime libraries.

./install-tool --build-cbtf-all 
--runtime-target-arch mic 
--cbtf-prefix /home/jgalaro/cbtf_only_stable 
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-cn-boost /home/jgalaro/krellroot_stable/compute 
--with-cn-mrnet /home/jgalaro/krellroot_stable/compute 
--with-cn-xercesc /home/jgalaro/krellroot_stable/compute 
--with-cn-libmonitor /home/jgalaro/krellroot_stable/compute 
--with-cn-libunwind /home/jgalaro/krellroot_stable/compute 
--with-cn-dyninst home/jgalaro/krellroot_stable/compute 
--with-cn-cbtf-krell /home/jgalaro/cbtf_only_stable/compute 
--with-cn-cbtf /home/jgalaro/cbtf_only_stable/compute 
--with-binutils /home/jgalaro/krellroot_stable 
--with-boost /home/jgalaro/krellroot_stable 
--with-mrnet /home/jgalaro/krellroot_stable 
--with-xercesc /home/jgalaro/krellroot_stable 
--with-libmonitor /home/jgalaro/krellroot_stable 
--with-libunwind /home/jgalaro/krellroot_stable 
--with-dyninst /home/jgalaro/krellroot_stable 
Build the front-end node version of O|SS for the CBTF version
./install-tool --target-arch cray 
--build-oss 
--openss-prefix /home/jgalaro/osscbtf_stable 
--with-cn-cbtf-krell /home/jgalaro/cbtf_only_stable/compute 
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-mpich2 /opt/cray/mpt/7.0.1/gni/mpich2-gnu/48 
--with-papi /opt/cray/papi/5.3.1 
--with-boost /home/jgalaro/krellroot_stable 
--with-mrnet /home/jgalaro/krellroot_stable 
--with-xercesc /home/jgalaro/krellroot_stable 
--with-libmonitor /home/jgalaro/krellroot_stable 
--with-libunwind /home/jgalaro/krellroot_stable 
--with-dyninst /home/jgalaro/krellroot_stable 
--with-libelf /home/jgalaro/krellroot_stable 
--with-libdwarf /home/jgalaro/krellroot_stable 
--with-binutils /home/jgalaro/krellroot_stable 
--cbtf-prefix /home/jgalaro/cbtf_only_stable

ARM Platform Install Examples

The cmake tool is also required by the cbtf build and is not loaded by default on many systems.  A possible module load scenario prior to invoking the install-tool scripts is as follows:
module load cmake

Instructions for O|SS CBTF based versions on ARM platforms

Basically building for the ARM is similar to building for a generic cluster, except that we need to know that we are building for the ARM in order to add some special compiler options (-funwind-tables -fasynchronous-unwind-tables) for unwinding call paths.  So, we ask for builders to specify the “--target-arch arm” phrase to the “install-tool” build commands.
Building for the ARM platform
Setup up the build environment for building for the front-end or login nodes
* module load cmake

Build the krellroot - components needed to support building cbtf and O|SS

./install-tool --build-krell-root 
--target-arch arm
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-openmpi /home/projects/arm64/openmpi/1.8.2/gnu/4.9.1 
--with-qt3 /home/jgalaro/qt3
Build cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl

./install-tool --build-cbtf-all 
--target-arch arm
--cbtf-prefix /home/jgalaro/cbtf_only_stable
--krell-root-prefix /home/jgalaro/krellroot_stable
--with-openmpi /home/projects/arm64/openmpi/1.8.2/gnu/4.9.1

Build O|SS for the CBTF version
./install-tool --build-oss
--target-arch arm
--cbtf-prefix /home/jgalaro/cbtf_only_stable
--krell-root-prefix /home/jgalaro/krellroot_stable
--openss-prefix /home/jgalaro/osscbtf_stable
--with-openmpi /home/projects/arm64/openmpi/1.8.2/gnu/4.9.1



Power 8 Platform Install Examples

The cmake tool is also required by the cbtf build and is not loaded by default on many systems.  A possible module load scenario prior to invoking the install-tool scripts is as follows:
module load cmake

Instructions for O|SS CBTF based versions on Power 8 platforms

Basically building for the Power 8 is similar to building for a generic cluster, except that we need to know that we are building for the POWER8.  So, we ask for builders to specify the “--target-arch power8” phrase to the “install-tool” build commands.
Building for the POWER8 platform
Setup up the build environment for building for the front-end or login nodes
* module load cmake

Build the krellroot - components needed to support building cbtf and O|SS

./install-tool --build-krell-root 
--target-arch power8
--krell-root-prefix /home/jgalaro/krellroot_stable 
--with-openmpi <openmpi install location> 
--with-qt3 /home/jgalaro/qt3
Build cbtf, cbtf-krell, cbtf-argonavis, cbtf-lanl

./install-tool --build-cbtf-all 
--target-arch power8
--cbtf-prefix /home/jgalaro/cbtf_ stable
--krell-root-prefix /home/jgalaro/krellroot_stable
--with-openmpi <openmpi install location>

Build O|SS for the CBTF version
./install-tool --build-oss
--target-arch power8
--cbtf-prefix /home/jgalaro/cbtf_stable
--krell-root-prefix /home/jgalaro/krellroot_stable
--openss-prefix /home/jgalaro/osscbtf_stable
              --with-openmpi <openmpi install location> 

To Run O|SS

Please refer to the Quick Start Guide from the O|SS documentation web-site page:
https://www.openspeedshop.org/wp/documentation/
for a short introduction to how to use O|SS.

For access the Users Guide from the O|SS documentation web-site page:

https://www.openspeedshop.org/wp/documentation/

These are the best sources for information on how to run O|SS.

Environment Setup

OPENSS_PLUGIN_PATH

This environment variable specifies where the *O|SS* main program will look for experiment plugins.  This is in addition to the normal search path, which is "<installdir>/lib{64}/openspeedshop” Prior to O|SS initialization set this variable to the path to your non-default plugins, (e.g. "setenv OPENSS_PLUGIN_PATH /g2/install/lib/openspeedshop)

LD_LIBRARY_PATH

This environment variable points to the directories where O|SS component libraries and O|SS libraries are installed.  
Set this environment variable to <installdir>/lib{64} (e.g. "setenv LD_LIBRARY_PATH <installdir>/lib{64}:$LD_LIBRARY_PATH)
DYNINSTAPI_RT_LIB

This environment variable points to the directory where the Dyninst (dynamic runtime library) component runtime library is installed.  This library is used in O|SS to detect loops and create per-loop statistic performance information available in the O|SS views.
Set this environment variable to 
                          <installdir for dyninst>/lib{64}/libdyninstAPI_RT.so 
For example:
     setenv DYNINSTAPI_RT_LIB <installdir for dyninst>/lib{64/libdyninstAPI_RT.so
OPENSS_MPI_IMPLEMENTATION (for offline mode of operation)

This environment variable specifies the MPI implementation being used by the MPI application whose performance is being analyzed.  Should be set to one of the currently supported MPI implementations:

* mpich
* mpich2
* mpt
* mvapich
* mvapich2
* openmpi


O|SS can auto-detect most of the MPI implementations that an
MPI application is using.  So, this variable will only be used to override the auto-detection code, if need be.    NOTE: If running the CBTF version of O|SS, please set CBTF_MPI_IMPLEMENTATION because O|SS is using CBTF to gather the performance information and that tool requires CBTF_MPI_IMPLEMENTATION to be set.

PATH

This environment variable specifies the path to the O|SS component executables and to the O|SS executables.  Set this to:  <installdir>/bin (e.g. setenv PATH <installdir>/bin:$PATH).

OPENSS_RAWDATA_DIR (offline mode of operation only)

This environment variable specifies the path to where the O|SS offline mode of operation will store the rawdata files representing the performance data that was gathered from the user application.   This is needed on clusters where the default rawdata file directory, /tmp, is not shared across the nodes.   If this is the case on the cluster you are running the offline mode of operation (for example: osspcsamp –offline …), then OPENSS_RAWDATA_DIR must be set to a shared file system directory.  Otherwise, /tmp is used.    Set this to:  
      OPENSS_RAWDATA_DIR <path_to_subdir_for_rawdata_file_writing>  
(e.g. setenv OPENSS_RAWDATA_DIR /p/lscratchc/jeg).

OPENSS_DB_DIR

This environment variable specifies the path to where the offline version of openss will build the O|SS database file. If you are on a file system that does not have file locking turned on, the sqlite component will not be able to create the database file.  You can use this environment variable to specify a path to a file system that does have locking turned to be used for the database file creation.


Runtime Environment Examples

The following runtime environment examples can be used as examples for creating module, dotkit, or softenv files depending on your systems execution environment.
Generic Cluster Module File Example

If your system supports the environment module file runtime environment setup, 
then these examples will provide some guidance for your O|SS runtime environment file creation.
O|SS module file example

#%Module1.0#####################################################################
## openss modulefile
##
proc ModulesHelp { } {
        global version openss
        puts stderr "\topenss - loads the O|SS software & application environment"
        puts stderr "\n\tThis adds $oss/* to several of the"
        puts stderr "\tenvironment variables."
        puts stderr "\n\tVersion $version\n"
}
module-whatis   "loads the O|SS runtime environment"
# for Tcl script use only
set     version         2.3.0
set     oss        /opt/OSS-2.3.0
set     kroot        /opt/krellroot
set     cbtf        /opt/cbtf_only-1.1

setenv          OPENSS_DOC_DIR		$oss/share/doc/packages/OpenSpeedShop
prepend-path    PATH			$kroot/bin
prepend-path    PATH			$cbtf/bin
prepend-path    PATH			$cbtf/sbin
prepend-path    PATH			$oss/bin
prepend-path    MANPATH		$oss/share/man

set unameexe  "/bin/uname"
if { [file exists $unameexe] } {
    set machinetype [ exec /bin/uname -m ]
    if { $machinetype == "x86" ||
         $machinetype == "i386" ||
         $machinetype == "i486" ||
         $machinetype == "i586" ||
         $machinetype == "i686" } {
        setenv OPENSS_PLUGIN_PATH   $oss/lib/openspeedshop
        setenv DYNINSTAPI_RT_LIB        $kroot/lib/libdyninstAPI_RT.so
        prepend-path LD_LIBRARY_PATH $kroot/lib
        prepend-path LD_LIBRARY_PATH $cbtf/lib
        prepend-path LD_LIBRARY_PATH $oss/lib

    }
    if { $machinetype == "x86_64" } {
        setenv OPENSS_PLUGIN_PATH    $oss/lib64/openspeedshop
        setenv DYNINSTAPI_RT_LIB     $kroot/lib64/libdyninstAPI_RT.so
        prepend-path LD_LIBRARY_PATH $kroot/lib64
        prepend-path LD_LIBRARY_PATH $kroot/lib
        prepend-path LD_LIBRARY_PATH $cbtf/lib64
        prepend-path LD_LIBRARY_PATH $oss/lib64
    }
    if { $machinetype == "ia64" } {
        setenv OPENSS_PLUGIN_PATH   $oss/lib/openspeedshop
        setenv DYNINSTAPI_RT_LIB        $kroot/lib/libdyninstAPI_RT.so
        prepend-path LD_LIBRARY_PATH $kroot/lib
        prepend-path LD_LIBRARY_PATH $cbtf/lib
        prepend-path LD_LIBRARY_PATH $oss/lib
    }
}

Generic Cluster Dotkit File Example

This is an example of a dotkit file used for a 64-bit cluster platform installation and is not generalized to support different platforms other than the 64-bit cluster it was written for.  Use the "use <filename of dotkit file>" command to activate the O|SS runtime environment.   Note: do not include the ".dk" portion of the filename when using the "use" command.

O|SS dotkit example

#c performance/profile
#d O|SS (Version 2.3.0)
dk_setenv OPENSS_PREFIX /usr/global/tools/openspeedshop/oss-dev/OSS-2.3.0
dk_setenv KROOT /usr/global/tools/openspeedshop/oss-dev/krellroot
dk_setenv CBTF /usr/global/tools/openspeedshop/oss-dev/cbtf_only-1.1

dk_setenv OPENSS_PLUGIN_PATH $OPENSS_PREFIX/lib64/openspeedshop
dk_setenv OPENSS_DOC $OPENSS_PREFIX/share/doc/packages/OpenSpeedShop/

# Find Dyninst for generation of per-loop statistics
dk_setenv DYNINSTAPI_RT_LIB        $KROOT/lib64/libdyninstAPI_RT.so

dk_alter PATH            $KROOT/bin
dk_alter LD_LIBRARY_PATH $KROOT/lib64

dk_alter PATH            $CBTF/bin 
dk_alter PATH            $CBTF/sbin
dk_alter LD_LIBRARY_PATH $CBTF/lib64

dk_alter PATH            $OPENSS_PREFIX/bin
dk_alter LD_LIBRARY_PATH $OPENSS_PREFIX/lib64

dk_setenv OPENSS_MPI_IMPLEMENTATION mvapich

Blue Gene/Q Softenv File Example
Offline version (no krell root usage)

This is an example of a softenv file used for a Blue Gene/Q installation.  Use the "resoft <filename of softenv file>" command to activate the O|SS runtime environment.  Note that the installation paths in this file follow the convention where compute node version of O|SS is installed as a "bgq" subdirectory of the installation path for the viewer version.    For illustration:
* Viewer version installed in: /home/projects/oss/oss
* Compute node version installed in: /home/projects/oss/oss/bgq

# The O|SS .soft file.
# Remember to type "resoft" after working on this file.

OSS = /home/projects/oss/oss
TARCH = bgq

# Set up OSS environment variables

# Find the executable portions of O|SS (order is important here)
PATH += $OSS/$TARCH/bin
PATH += $OSS/bin

# Find the libraries for O|SS (order is important here)
LD_LIBRARY_PATH += $OSS/$TARCH/lib64
LD_LIBRARY_PATH += $OSS/lib64

# Find the runtime collectors
OPENSS_PLUGIN_PATH = $OSS/$TARCH/lib64/openspeedshop

# Find Dyninst for generation of per-loop statistics
DYNINSTAPI_RT_LIB $OSS/lib64/libdyninstAPI_RT.so

# Tell the tool what the application MPI implementation is
# Needed if supporting multiple implementations and running the "mpi", "mpit", or "mpiotf" experiments
OPENSS_MPI_IMPLEMENTATION = mpich2

# Paths to documentation and man pages
OPENSS_DOC_DIR = $OSS/share/doc/packages/OpenSpeedShop
MANPATH = $OSS/share/man

# Use the basic environment.
@default
Offline version (with krell root usage)

# The O|SS .soft file.
# Remember to type "resoft" after working on this file.

OSS = /home/projects/oss/oss
KROOT = /home/projects/krellroot
TARCH = bgq

# Set up OSS environment variables

# Find the executable portions of O|SS (order is important here)
PATH += $KROOT/$TARCH/bin
PATH += $KROOT/bin
PATH += $OSS/$TARCH/bin
PATH += $OSS/bin

# Find the libraries for O|SS (order is important here)
LD_LIBRARY_PATH += $KROOT/$TARCH/lib64
LD_LIBRARY_PATH += $KROOT/lib64
LD_LIBRARY_PATH += $OSS/$TARCH/lib64
LD_LIBRARY_PATH += $OSS/lib64

# Find the runtime collectors
OPENSS_PLUGIN_PATH = $OSS/$TARCH/lib64/openspeedshop

# Find Dyninst for generation of per-loop statistics
DYNINSTAPI_RT_LIB  $KROOT/lib64/libdyninstAPI_RT.so

# Tell the tool what the application MPI implementation is
# Needed if supporting multiple implementations and running the "mpi", "mpit", or "mpiotf" experiments
OPENSS_MPI_IMPLEMENTATION = mpich2

# Paths to documentation and man pages
OPENSS_DOC_DIR = $OSS/share/doc/packages/OpenSpeedShop
MANPATH = $OSS/share/man

# Use the basic environment.
@default

Cray Platform Module File Example

If your system supports the environment module file runtime environment setup, then these examples will provide some guidance for your O|SS runtime environment file creation.   Note that the compiler library path was prepended to the LD_LIBRARY_PATH below.  That is necessary if building with a compiler that is installed in a non-standard location (a module file was loaded).  O|SS needs to find the libstdc++ and libgomp libraries that the tool was built with when the tool executes.
Cray platform CBTF version module file example

#%Module1.0##################################################################### 
## 
## osscbtf modulefile 
## 
proc ModulesHelp { } { 
        global version osscbtf 

        puts stderr "\topenss - loads the OpenSpeedShop software & application environment" 
        puts stderr "\n\tThis adds $oss/* to several of the" 
        puts stderr "\tenvironment variables." 
        puts stderr "\n\tVersion $version\n" 
} 

module-whatis   "Loads the OpenSpeedShop CBTF instrumentor based runtime environment." 

# for Tcl script use only 
set     version         2.3.0.beta1 
set     root /p/home/app/PET/pkgs/openss/krellroot_v2.3.0.beta1 
set     dynroot /p/home/app/PET/pkgs/openss/krellroot_v2.3.0.beta1 
set     boostroot /p/home/app/PET/pkgs/openss/krellroot_v2.3.0.beta1 
set     mrnetroot /p/home/app/PET/pkgs/openss/krellroot_v2.3.0.beta1 
set     cbtf /p/home/app/PET/pkgs/openss/cbtf_v2.3.0.beta1 
set     cbtfk /p/home/app/PET/pkgs/openss/cbtf_v2.3.0.beta1 
set     oss /p/home/app/PET/pkgs/openss/osscbtf_v2.3.0.beta1 
set     stdcpp          /opt/gcc/4.9.2/snos 

setenv          CBTF_MPI_IMPLEMENTATION    mpich 
setenv          XPLAT_RSH       ssh 

# only need these for situations where the environment is not passed 
setenv          MRNET_COMM_PATH $cbtfk/sbin/cbtf_mrnet_commnode 
setenv          CBTF_MRNET_BACKEND_PATH $cbtfk/sbin/cbtf_libcbtf_mrnet_backend 

prepend-path    PATH            $cbtfk/sbin 
prepend-path    PATH            $cbtfk/bin 
prepend-path    PATH            $oss/bin 
prepend-path    MANPATH            $oss/share/man 

eval set  [ array get env HOME ] 
set     ownmoddir       $HOME/privatemodules 

setenv DYNINSTAPI_RT_LIB $dynroot/lib64/libdyninstAPI_RT.so 

prepend-path LD_LIBRARY_PATH $dynroot/lib64 
prepend-path LD_LIBRARY_PATH $root/lib64 
prepend-path LD_LIBRARY_PATH $mrnetroot/lib64 
prepend-path LD_LIBRARY_PATH $oss/lib64 
prepend-path LD_LIBRARY_PATH $cbtf/lib64 
prepend-path LD_LIBRARY_PATH $cbtfk/lib64 
prepend-path LD_LIBRARY_PATH $boostroot/lib 
prepend-path LD_LIBRARY_PATH $stdcpp/lib64


Intel MIC Platform Module File Examples

If your system supports the environment module file runtime environment setup, then these examples will provide some guidance for your O|SS runtime environment file creation.


Intel MIC (KNL) platform CBTF version module file example

Use the generic Generic Cluster Module File Example shown in the example above for KNL installations.


Intel MIC (KNC co-processor) platform CBTF version module file example

#%Module1.0#####################################################################
##
## osscbtf modulefile
##
proc ModulesHelp { } {
        global version osscbtf

        puts stderr "\topenss - loads the O|SS software & application environment"
        puts stderr "\n\tThis adds $oss/* to several of the"
        puts stderr "\tenvironment variables."
        puts stderr "\n\tVersion $version\n"
}

module-whatis   "Loads the O|SS CBTF instrumentor based runtime environment."

# for Tcl script use only
set     version         2.3.0
set     root            /global/u2/j/jgalaro/cori/krellroot_stable
set     dynroot         /global/u2/j/jgalaro/cori/krellroot_stable
set     boostroot       /global/u2/j/jgalaro/cori/krellroot_stable
set     mrnetroot       /global/u2/j/jgalaro/cori/krellroot_stable
set     cbtf /global/u2/j/jgalaro/cori/jgalaro/cbtf_only_stable
set     cbtfk /global/u2/j/jgalaro/cori/jgalaro/cbtf_only_stable
set     oss /global/u2/j/jgalaro/cori/jgalaro/osscbtf_stable
set     stdcpp          /opt/gcc/5.1.0/snos/

setenv          CBTF_MPI_IMPLEMENTATION    mpich2
setenv          XPLAT_RSH       ssh

# only need these for situations where the environment is not passed
setenv          MRNET_COMM_PATH $cbtfk/sbin/cbtf_mrnet_commnode
setenv          CBTF_MRNET_BACKEND_PATH $cbtfk/sbin/cbtf_libcbtf_mrnet_backend

prepend-path    PATH            $cbtfk/sbin
prepend-path    PATH            $cbtfk/bin
prepend-path    PATH            $oss/bin
prepend-path    MANPATH            $oss/share/man

eval set [ array get env HOME]
set     ownmoddir       $HOME/privatemodules

setenv DYNINSTAPI_RT_LIB $dynroot/lib64/libdyninstAPI_RT.so

prepend-path LD_LIBRARY_PATH $dynroot/lib64
prepend-path LD_LIBRARY_PATH $oss/lib64
prepend-path LD_LIBRARY_PATH $cbtf/lib64
prepend-path LD_LIBRARY_PATH $cbtfk/lib64

# Because the module for the gcc was used to build OSS we need to make the libstdc++.so.6
# available when we are running OSS
prepend-path LD_LIBRARY_PATH $stdcpp/lib64


ARM platform CBTF version module file example

Use the generic Generic Cluster Module File Example shown in the example above for ARM installations.

Power 8 platform CBTF version module file example

Use the generic Generic Cluster Module File Example shown in the example above for Power 8 installations.


How to use SPACK to build O|SS

To be able to use spack to build O|SS, you must install spack on your system.  To find the latest release go to this web page to find the current releases.
    https://github.com/LLNL/spack/releases
The most current release at this point can be downloaded using this url:
    https://github.com/LLNL/spack/archive/v0.10.0.tar.gz 

Once, spack is downloaded, use:
tar -zxf <tarball_name>.tar.gz 
to unpack it.
Then change directories into the spack top-level directory.
Use these commands to set up spack for use:
export SPACK_ROOT=<path to spack>/LLNL-spack-52a9e5d 
export PATH=$SPACK_ROOT/bin:$PATH

Now spack is ready to accept commands.   There are many commands that help spack users to customize their builds.   But, O|SS can be built under ideal circumstances with this command:
spack install openspeedshop

This command executes the O|SS package files that were created by the O|SS developers which detail the dependencies which are needed for a successful build of O|SS.  So, there are many other components, which also have spack package files and need to be built before O|SS can be finally built.   

The above command builds an O|SS installation without support for the MPI experiments (mpi, mpit, mpip).   So, to get that support, spack supports variants.  These are optional flags that indicate to the spack packages to do the optional action corresponding to the variant specification.  There is a variant for the various MPI implementations supported by O|SS.

spack install openspeedshop +mpich +openmpi

will build or find mpich and openmpi MPI implementations and create support for the MPI collectors (mpi, mpit, mpip) for O|SS.

Spack creates module files for O|SS and other packages

Spack automatically creates a module file that can and should be used to set up the runtime environment for O|SS.   
The spack created module files can be found in this relative location:
$SPACK_ROOT/share/spack/modules/<machine_type>/....

1 MPT is transitioning to MPI-3 support but it is not completed.  Please use MPT-2.08 and above when building the MPI collectors.  We have added special code to support their implementation of mpi.h MPI Function specifications with our wrappers.
---------------

------------------------------------------------------------

---------------

------------------------------------------------------------




23


