
This represents a new OpenSpeedShop-2.4.0 release.

FEATURE 1:

One of the main features added is the cbtfsummary high-level performance experiment.
The cbtfsummary experiment is designed to work on sequential, MPI, OpenMP, and hybrid
codes and directly leverages several interfaces available for tools inside current MPI
implementations like, MVAPICH and OpenMPI. The goal is to provide its user with a high-level
view of the performance of their application.
The cbtfsummary experiment gathers high-level information for a number of performance metrics,
such as:
* Host, rank, thread, application meta-data
* Time spent in MPI routines
* Time spent in OpenMP (idle time, barrier time, task time)
* Hardware counters (multiplexes HW counters)
* Time spent in I/O (breaks down read and write times and byte totals)
* Memory information
    o Allocation calls, bytes, and time
    o Free calls and time
    o PAPI dmem and statistics including high water mark
    o rusage max rss
* rusage utime and stime

The cbtfsummary experiment outputs a report in human readable form, in addition to the csv files that
are created when cbtfsummary is finished running. To generate the initial csv files, a final .txt report
file (report is also printed to stdout), and a master csv file (a concatenation of all the individual csv
files), the following command is used:
cbtfsummary “how you run your application normally”
For example, with MPI:
cbtsummary “mpirun –np <number of ranks> <application><application arguments>"

FEATURE 2:

The original effort was to detect Intel instructions that have 512 vector length arguments, i.e. AVX512 instructions.   
We broadened the original objective and now detect vector instructions in general and can report vector instructions 
of various vector lengths , where AVX512 is a special case of the general detection.   This gives the Open|SpeedShop 
tool flexibility to handle possible future architectural changes and report more categories for existing systems.
This functionality was implemented by using the gathered/sampled addresses to search through the executable during 
post processing the executable using the Dyninst API.   The sampled addresses, which were gathered during the 
application execution, are matched with instruction at that address .  The instruction must be one that is in the 
Dyninst vector category.  The operands are examined for their size and the largest size is recorded and subsequently 
reporting in the Open|SpeedShop CLI.    The new view is displayed via the “expview -vvectorinstr”.
This work was developed and tested on a number of machines including three KNL platforms and various  laboratory x86_64 platforms.
There is no special options to gather this information.   The vector instructions will be detected if they are present on 
Intel based plaforms.
Here is a short example of the output.   Note the same default view metrics are shown along with the instruction 
mnemonics, instruction address, and the maximum size of the operands for the instruction.

dgemm application:

openss -cli -f dgemm_onyx_knl-pcsamp-2.openss
openss>>[openss]: The restored experiment identifier is:  -x 1
openss>>expview -vvectorinstr

Exclusive   % of CPU  Vector Instr Location (Line Number/Addr : OpCode  : Max Operand Size (bits))
 CPU time       Time  
       in             
 seconds.             
 0.460000  10.874704  0x20001f94 (run : dgemm_onyx_knl) : dgemm.c(188) : movsd %xmm8,0x0(%rsp,%rsi,8) : 128
 0.450000  10.638298  0x20001f8e (run : dgemm_onyx_knl) : dgemm.c(187) : movsd %xmm8,0x0(%rdi,%rsi,8) : 128
 0.380000   8.983452  0x20001f50 (run : dgemm_onyx_knl) : dgemm.c(187) : vcvtsi2sd 0xffffffffffffffe0(%rbp),%xmm0 : 128
 0.360000   8.510638  0x200014e9 (main : dgemm_onyx_knl) : dgemm.c(187) : vcvtsi2sd 0xffffffffffffffe8(%rbp),%xmm0 : 128
 0.240000   5.673759  0x20001527 (main : dgemm_onyx_knl) : dgemm.c(187) : movsd %xmm8,0x0(%rsp,%rbp,8) : 128
 0.230000   5.437352  0x2000151b (main : dgemm_onyx_knl) : dgemm.c(187) : vfmadd231sd %xmm4,%xmm5,%xmm6{%k0} : 128
 0.220000   5.200946  0x20001521 (main : dgemm_onyx_knl) : dgemm.c(187) : vmaskmovpd %xmm6,%xmm7,%xmm8{%k0} : 128
 0.190000   4.491726  0x2000152d (main : dgemm_onyx_knl) : dgemm.c(188) : movsd %xmm8,0x0(%rdi,%rbp,8) : 128
 0.180000   4.255319  0x20001f9a (run : dgemm_onyx_knl) : dgemm.c(189) : movsd %xmm8,0x0(%rbp,%rsi,8) : 128
 0.170000   4.018913  0x20001f55 (run : dgemm_onyx_knl) : dgemm.c(187) : vmovsd %xmm0,0xffffffffffffffd0(%rbp),%xmm2 : 128
 0.170000   4.018913  0x20001f88 (run : dgemm_onyx_knl) : dgemm.c(187) : vmaskmovpd %xmm6,%xmm7,%xmm8{%k0} : 128
 0.170000   4.018913  0x200014ee (main : dgemm_onyx_knl) : dgemm.c(187) : vmovsd %xmm0,0xffffffffffffffd8(%rbp),%xmm2 : 128
 0.170000   4.018913  0x20001f77 (run : dgemm_onyx_knl) : dgemm.c(187) : vsubsd %xmm1,0xffffffffffffffc0(%rbp),%xmm7 : 128
 0.160000   3.782506  0x200014fa (main : dgemm_onyx_knl) : dgemm.c(187) : vmovsd %xmm0,0xffffffffffffffd0(%rbp),%xmm3 : 128
 0.160000   3.782506  0x20001f61 (run : dgemm_onyx_knl) : dgemm.c(187) : vmovsd %xmm0,0xffffffffffffffc8(%rbp),%xmm3 : 128
 0.150000   3.546099  0x20001f82 (run : dgemm_onyx_knl) : dgemm.c(187) : vfmadd231sd %xmm4,%xmm5,%xmm6{%k0} : 128
 0.080000   1.891253  0x20001510 (main : dgemm_onyx_knl) : dgemm.c(187) : vsubsd %xmm1,0xffffffffffffffc0(%rbp),%xmm7 : 128
 0.060000   1.418440  0x20001533 (main : dgemm_onyx_knl) : dgemm.c(189) : movsd %xmm8,0x0(%rsi,%rbp,8) : 128
 0.030000   0.709220  0x20001f66 (run : dgemm_onyx_knl) : dgemm.c(187) : vgetexpsd %xmm0,%xmm0,%xmm1{%k0} : 128
 0.030000   0.709220  0x200014f3 (main : dgemm_onyx_knl) : dgemm.c(187) : vgetmantss %xmm0,%xmm0,0,%xmm4{%k0} : 128
 0.020000   0.472813  0x20001515 (main : dgemm_onyx_knl) : dgemm.c(187) : vfnmsub231sd %xmm6,%xmm3,%xmm4{%k0} : 128
 0.020000   0.472813  0x200014ff (main : dgemm_onyx_knl) : dgemm.c(187) : vgetexpsd %xmm0,%xmm0,%xmm1{%k0} : 128
 0.020000   0.472813  0x2000150a (main : dgemm_onyx_knl) : dgemm.c(187) : vmulsd %xmm2,%xmm4,%xmm6{%k0} : 128
 0.010000   0.236407  0x00002aaae0b59284 (mkl_blas_avx512_mic_dgemm_dcopy_down8_ea : libmkl_avx512_mic.so) : movupd %zmm0,0x5(%rcx){%k0} : 512
 0.010000   0.236407  0x00002aaae0b591f6 (mkl_blas_avx512_mic_dgemm_dcopy_down8_ea : libmkl_avx512_mic.so) : movupd %zmm0,0x1(%rcx){%k0} : 512
 0.010000   0.236407  0x00002aaae0b5923c (mkl_blas_avx512_mic_dgemm_dcopy_down8_ea : libmkl_avx512_mic.so) : movupd %zmm0,0x3(%rcx){%k0} : 512
 0.010000   0.236407  0x00002aaae0b59260 (mkl_blas_avx512_mic_dgemm_dcopy_down8_ea : libmkl_avx512_mic.so) : movupd %zmm0,0x4(%rcx){%k0} : 512
 0.010000   0.236407  0x00002aaae0b5927c (mkl_blas_avx512_mic_dgemm_dcopy_down8_ea : libmkl_avx512_mic.so) : vmovupd %zmm0,0xfffffffffffffffe(%rax,%rbx,1),%zmm0{%k0} : 512
 0.010000   0.236407  0x200014e5 (main : dgemm_onyx_knl) : dgemm.c(187) : vxorpd %xmm0,%xmm0,%xmm0 : 128
 0.010000   0.236407  0x00002aaae0b59184 (mkl_blas_avx512_mic_dgemm_dcopy_down8_ea : libmkl_avx512_mic.so) : vmovupd %zmm0,0xfffffffffffffffe(%rdx),%zmm0{%k0} : 512
 0.010000   0.236407  0x20001f7c (run : dgemm_onyx_knl) : dgemm.c(187) : vfnmsub231sd %xmm6,%xmm3,%xmm4{%k0} : 128
 0.010000   0.236407  0x20001f71 (run : dgemm_onyx_knl) : dgemm.c(187) : vmulsd %xmm2,%xmm4,%xmm6{%k0} : 128
 0.010000   0.236407  0x20001f5a (run : dgemm_onyx_knl) : dgemm.c(187) : vgetmantss %xmm0,%xmm0,0,%xmm4{%k0} : 128
 0.010000   0.236407  0x20001f4c (run : dgemm_onyx_knl) : dgemm.c(187) : vxorpd %xmm0,%xmm0,%xmm0 : 128
openss>>   


FEATURE 3:

Update the O|SS/CBTF build mechanisms to use the new SPACK support for the Cray platform and other non-generic platforms.  
Additional SPACK features are currently being developed to support building applications and tools on the Cray platform.  
We used those new SPACK features to adapt the current O|SS/CBTF spack package files to support 
building on the Cray platform while focusing on the Trinity platform.

FEATURE 4:

Turn off loop detection and vector instruction detection
by default and add options to the convenience scripts that
allows users to turn on loop detection and vector instruction
detection when desired.

The new options are as follows:
# Enable the executable post-processing for loop detection
      --loops)
# Disable the executable post-processing for loop detection
       --noloops)
# Enable the executable post-processing for vector instruction
# detection for instructions with 128 bit or larger operands
       --vinstr128)
# Enable the executable post-processing for vector instruction
# detection for instructions with 256 bit or larger operands
       --vinstr256)
# Enable the executable post-processing for vector instruction
# detection for instructions with 512 bit or larger operands
       --vinstr512)

Also add help documentation in the convenience scripts
so users are aware of the options.  Provide messages when
the scripts are running, so users are are that detection
is on or off.

FEATURE 5:

Better inline support for C++ - trying to help Kokkos and RAJA developers, as well
as better general C++ inline reporting.

The example call-stack below is from lulesh2.0.3 and was run on  grizzley at LANL.  This call-stack 
illustrates the relationship between the function doing the inlining function and the inlined functions.  
The call-stack below chains together the inlined functions as they occurred in the execution of the 
program based on sampling information collected by O|SS.   Note that the inlined and inlining 
function have the same number of indentation arrows to indicate the occurrence of inlining.

This call-stack is one of the many that were collected and displayed by the usertime experiment:

           _start (lulesh2.0)
           > @ 556 in __libc_start_main (libmonitor.so.0.0.0)
           >> @ 274 in __libc_start_main (libc-2.17.so)
           >>> @ 517 in monitor_main (libmonitor.so.0.0.0)
           >>>> @ 183 in main (lulesh2.0: lulesh.cc,2690)
           >>>> @ 2774 in LagrangeLeapFrog (lulesh2.0: lulesh.cc)
           >>>> @ 2656 in LagrangeElements (lulesh2.0: lulesh.cc)
           >>>> @ 2458 in CalcLagrangeElements (lulesh2.0: lulesh.cc)
           >>>> @ 1609 in CalcKinematicsForElems (lulesh2.0: lulesh.cc)
           >>>>> @ 2374 in __kmp_join_call (libomp.so)
           >>>>>> @ 7165 in __kmp_internal_join (libomp.so)
           >>>>>>> @ 334 in __kmp_join_barrier(int) (libomp.so)
    89  >>>>>>>> @ 167 in OMPT_THREAD_WAIT_BARRIER (collector.c,167)
       
       The chain of events that are illustrated above:
       At line 2774 of lulesh.cc,  LagrangeLeapFrog was inlined.
       At line 2656 in LagrangeLeapFrog,  LagrangeElements was inlined
       At line 2458 in  LagrangeElements, CalcLagrangeElements was inlined
       At line 1609 in CalcLagrangeElements, CalcKinematicsForElems was inlined
       
       Source excerpts from lulesh.cc from lulesh2.0.3:
         1598	/******************************************/
         1599	
         1600	static inline
         1601	void CalcLagrangeElements(Domain& domain, Real_t* vnew)
         1602	{
         1603	   Index_t numElem = domain.numElem() ;
         1604	   if (numElem > 0) {
         1605	      const Real_t deltatime = domain.deltatime() ;
         1606	
         1607	      domain.AllocateStrains(numElem);
         1608	
         1609	      CalcKinematicsForElems(domain, vnew, deltatime, numElem) ;
         1610	
         1611	      // element loop to do some stuff not included in the elemlib function.
         1612	#pragma omp parallel for firstprivate(numElem)
         1613	      for ( Index_t k=0 ; k<numElem ; ++k )
         1614	      {
         1615	         // calc strain rate and apply as constraint (only done in FB element)
       …
       …
       
         1634	      }
         1635	      domain.DeallocateStrains();
         1636	   }
         1637	}
         1638	
       
       
         2451	/******************************************/
         2452	
         2453	static inline
         2454	void LagrangeElements(Domain& domain, Index_t numElem)
         2455	{
         2456	  Real_t *vnew = Allocate<Real_t>(numElem) ;  /* new relative vol -- temp */
         2457	
         2458	  CalcLagrangeElements(domain, vnew) ;
         2459	
         2460	  /* Calculate Q.  (Monotonic q option requires communication) */
         2461	  CalcQForElems(domain, vnew) ;
         2462	
         2463	  ApplyMaterialPropertiesForElems(domain, vnew) ;
         2464	
         2465	  UpdateVolumesForElems(domain, vnew,
         2466	                        domain.v_cut(), numElem) ;
         2467	
         2468	  Release(&vnew);
         2469	}
         2470	
       
         2638	
         2639	static inline
         2640	void LagrangeLeapFrog(Domain& domain)
         2641	{
         2642	#ifdef SEDOV_SYNC_POS_VEL_LATE
         2643	   Domain_member fieldData[6] ;
         2644	#endif
         2645	
         2646	   /* calculate nodal forces, accelerations, velocities, positions, with
         2647	    * applied boundary conditions and slide surface considerations */
         2648	   LagrangeNodal(domain);
         2649	
         2650	
         2651	#ifdef SEDOV_SYNC_POS_VEL_LATE
         2652	#endif
         2653	
         2654	   /* calculate element quantities (i.e. velocity gradient & q), and update
         2655	    * material states */
         2656	   LagrangeElements(domain, domain.numElem());
         2657	
         2658	#if USE_MPI   
         2659	#ifdef SEDOV_SYNC_POS_VEL_LATE
         2660	   CommRecv(domain, MSG_SYNC_POS_VEL, 6,
         2661	            domain.sizeX() + 1, domain.sizeY() + 1, domain.sizeZ() + 1,
         2662	            false, false) ;
         2663	
         2664	   fieldData[0] = &Domain::x ;
         2665	   fieldData[1] = &Domain::y ;
         2666	   fieldData[2] = &Domain::z ;
         2667	   fieldData[3] = &Domain::xd ;
         2668	   fieldData[4] = &Domain::yd ;
         2669	   fieldData[5] = &Domain::zd ;
         2670	   
         2671	   CommSend(domain, MSG_SYNC_POS_VEL, 6, fieldData,
         2672	            domain.sizeX() + 1, domain.sizeY() + 1, domain.sizeZ() + 1,
         2673	            false, false) ;
         2674	#endif
         2675	#endif   
         2676	
         2677	   CalcTimeConstraintsForElems(domain);
         2678	
         2679	#if USE_MPI   
         2680	#ifdef SEDOV_SYNC_POS_VEL_LATE
         2681	   CommSyncPosVel(domain) ;
         2682	#endif
         2683	#endif   
         2684	}
         2685	
         2686	
       
       
         2687	/******************************************/
         2688	
         2689	int main(int argc, char *argv[])
         2690	{
         2691	  Domain *locDom ;
         2692	   Int_t numRanks ;
         2693	   Int_t myRank ;
         2694	   struct cmdLineOpts opts;
         2695	
         2696	#if USE_MPI   
         2697	   Domain_member fieldData ;
         2698	
       …
       …
       
         2770	//      std::cout << "region" << i + 1<< "size" << locDom->regElemSize(i) <<std::endl;
         2771	   while((locDom->time() < locDom->stoptime()) && (locDom->cycle() < opts.its)) {
         2772	
         2773	      TimeIncrement(*locDom) ;
         2774	      LagrangeLeapFrog(*locDom) ;
         2775	
         2776	      if ((opts.showProg != 0) && (opts.quiet == 0) && (myRank == 0)) {
         2777	         printf("cycle = %d, time = %e, dt=%e\n",
         2778	                locDom->cycle(), double(locDom->time()), double(locDom->deltatime()) ) ;
         2779	      }
         2780	   }
       
Specific Kokkos Example:
BEFORE changes to better support C++ inlining – top time taking callstack from kokkos-mxm.host:

       openss>>expview -vfullstack -mcalls usertime1
       
       Number of  Call Stack Function (defining location)
       Exclusive  
          Counts  
                  _start (kokkos-mxm.host)
                  > @ 556 in __libc_start_main (libmonitor.so.0.0.0: main.c,541)
                  >>__libc_start_main (libc-2.26.so)
                  >>> @ 517 in monitor_main (libmonitor.so.0.0.0: main.c,492)
                  >>>> @ 131 in main (kokkos-mxm.host: kokkos-mxm.cpp,3)
                  >>>>> @ 1168 in __kmp_api_GOMP_parallel (libomp.so: kmp_gsupport.cpp,1136)
         158  >>>>>> @ 91 in Kokkos::Impl::ParallelFor<main::{lambda(int const&)#2}, Kokkos::RangePolicy<Kokkos::OpenMP>, Kokkos::RangePolicy>::execute() const [clone ._omp_fn.2] (kokkos-mxm.host: Kokkos_OpenMP_Parallel.hpp,131)
       
AFTER changes to better support C++ inlining – top time taking callstack from kokkos-mxm.host:
       
       openss>>expview -v fullstack -mcalls usertime1
       
       Number of  Call Stack Function (defining location)
       Exclusive  
          Counts  

           _start (kokkos-mxm.host)
           > @ 556 in __libc_start_main (libmonitor.so.0.0.0)
           >>__libc_start_main (libc-2.26.so)
           >>> @ 517 in monitor_main (libmonitor.so.0.0.0)
           >>>> @ 3 in main (kokkos-mxm.host: kokkos-mxm.cpp,3)
           >>>> @ 244 in parallel_for<main(int, char**)::<lambda(int const&)> > (kokkos-mxm.host: Kokkos_Parallel.hpp)
           >>>> @ 224 in execute (kokkos-mxm.host: Kokkos_Parallel.hpp)
           >>>> @ 20 in parallel_for<int, main(int, char**)::<lambda(int const&)> > (kokkos-mxm.host: kokkos-mxm.cpp)
           >>>>> @ 1168 in __kmp_api_GOMP_parallel (libomp.so)
  138  >>>>>> @ 91 in Kokkos::Impl::ParallelFor<main::{lambda(int const&)#2}, Kokkos::RangePolicy<Kokkos::OpenMP>, Kokkos::RangePolicy>::execute() const [clone ._omp_fn.2] (kokkos-mxm.host: Kokkos_OpenMP_Parallel.hpp,131)
         
       

This represents a new OpenSpeedShop-2.3.1 release.

---------------------------------------------------------------------
Here is a partial list of the changes since the 2.3.0 release.
---------------------------------------------------------------------

Fixes for the IBM power platform related to TLS and blob sizes.

Changes for supporting OpenMP-5.0 OMPT interface with the OpenSpeedShop sampling and omptp experiments on Tri-lab platforms.
Build llvm-openmp instead of ompt by default.
Added llvm-openmp to the OpenSpeedShop build in spack and modified the llvm-openmp spack package to build the library standalone so it can be linked into OpenSpeedShop.
Add more derived metric help text (help hwcsamp). 
Update the compare convenience script with more information.
Fixes for unwinding on powerpc64 platform. Affects sampling and tracing experiments. profiling experiments still work as usual.

There were a number of issues with the driver script that were discovered while debugging on serrano and then verified the same issues on another system.
The low, high, default flags for pcsamp and usertime did not work. For example: osspcsamp mpirun -np 2 ./nbody low just used the default value. 
This is because the settings for high_flag, low_flag, and default_value_flag are true or false, but the check were for 1 or 0. That apparently does not work.  This is fixed.
If we set CBTF_PCSAMP_RATE or CBTF_USERTIME_RATE, OSS used those values but reported to the user that it was using the default rate value. This is fixed."

Updated openspeedshop and cbtf-krell to be able to use binutils-2.28 as requested by Intel.  Comment out the bfd_set_error_handler code since it was not implemented and binutils-2.28 conflicts with the bfdErrHandler definition from the 2.23 and 2.24 binutils versions we had been using.  For OSS, Comment out the bfd_set_error_handler code since it was not implemented and binutils-2.28 conflicts with the bfdErrHandler definition from the 2.23 and 2.24 binutils versions we had been using.  Add the tarball to the ROOT source repository and update the build scripts.  Then test.

Fix two issues dicovered testing daemonTool at LANL.
1. Fix localhost topology generation where an extra level was created with no terminating be node. 
2. Fix handling of slurm allocations for 2 to N nodes. 
We should now create a tree that has leaf nodes for each node in an allocation and as many intermediate nodes (CP) for reduction nodes. Tested with slurm environment and a simple localhost test on laptop.

For openspeedshop, IBM named their libmpi library libmpi_ibm.  Add mpi_ibm to the library search names in the find_package file for OpenMPI.

Continue work on moving up to version 9.3.2 of Dyninst 9.2 for better Power 8 support.  This includes changes to build scripts and introducing using elfutils instead of libelf-0.8.13 and making sure libz is available.  Investigate DyninstAPI interface change that is impacting SymtabAPISymbols::getAllSymbols in core/src/SymtabAPISymbols.cpp.  Turned out that this code was dead code, so it was removed.

Updates all cmake builds to use the cmake way of specifying for position independent code (PIC).  Remove all remaining references to libtool (ltdl) code that where not needed.  Changed all collectors built by cbtf-krell to link dependent code (services,messages) from archive libraries (.a's) into the named collector type and adjusted cbtfrun.in to only preload the minimal needed for the collector to to work. Fixed a few more compiler warnings.

Disable EXPLICIT_TLS for mem collector.c when build is configured for TLS_MODEL=explicit due to malloc wrapper issues. Fix omptp collector callbacks to handle explicit tls model. Remove conflicting (and unused) function for destroying explicit tls in services/collector/collector.c since that code already handled free of explict tls memory

Remove older deprecated symbol code that was incompatible with 9.3.2 (method getAllSymbols).

Many cuda experiment related fixes and adjustments to follow the cbtf-krell communication patterns better.



NOTE: 2.3.1 has important fixes which prevented OpenSpeedShop from working on Omni-Path systems.
      We are still seeing some issues with usertime and hwctime experiments on Omni-Path systems.
      We are working on those issues as well and those fixes will be in the 2.3.1 official release.

      There are also changes for building MRNet with CTI options.  It appears Cray is moving toward using the CTI interface on new systems.
      See startup_files/dod_onyx_build.sh for the new options to use on those systems.   The look like this: (--use-cti and --with-cti).
      For example:

          ./install-tool --use-cti --runtime-only --target-arch cray --target-shared  \
                 --build-krell-root --krell-root-prefix ${KROOT_IDIR}/compute  \
                 --with-mpich ${MPICH_IDIR} --with-cti /opt/cray/pe/cti/1.0.4 


We have found these issues with the base 2.3.0 release. 
The issues are fixed in a 2.3.1 release.
We apologize if you have encountered any of these.

Several people requested we fix the python scripting API to be able to run experiments from it.  
This is fixed, see Issue 18 below.

Also, the module files for OSS/CBTF have to change from what they used to be for the offline versions.
More environment variables need to be set.   See the NOTE at the end of this file for the new module file example.

We now build cmake-3.2.2 as the first step of the build process.   New cmake versions break the OMPT component build, which is the first component built in the krellroot.  To prevent this failure, we decided to provide our own version of cmake, known to work in building OMPT until we can figure out the issue and report it to the cmake maintainers.

-------------------------------
Issues found and fixed:
-------------------------------

Issue 1: 
The --offline mode of operation in the CBTF build was apparently broken by a late fix for another issue.  This is fixed.

Issue 2: 
If an executable is passed to OpenSpeedShop but does not exist.  OpenSpeedShop now puts out an error message and exits and reports the issue to the user.  This is fixed.

Issue 3: 
The iot and mpip views had extraneous data output in the call path views in both the command line interface (CLI) and in the graphical user interface (GUI) views.   This is fixed.

Issue 4: 
Builds for the Cray for the default CBTF mode of operation were broken by the same late fix (see Issue 1).  This caused a runtime error that looked like this:

  what():  The specified plugin ("/project/projectdirs/m888/jgalaro/edison/openss/cbtf_v2.3.0.beta1/lib64/KrellInstitute/Components") doesn't exist or is not of the correct format. dlopen() reported "/project/projectdirs/m888/jgalaro/edison/openss/cbtf_v2.3.0.beta1/lib64/KrellInstitute/Components: cannot read file data: Is a directory".

This is fixed.

Issue 5: 
Turn off the debug from the OMPT interface which shows up when profiling OpenMP and OpenMP/MPI applications.

Issue 6:
Created an OSS Reference Manual by stripping out non-OSS information out of the Users Guide.  
Add pdf version of latest OSS Reference guide to the docs directory, so it can be accessed from the new GUI.
Add the latest version of the Quick Start Guide to the docs directory, so it can be accessed from the new GUI.

Issue 7:
Update the openss cmake file to also honor qopenmp which is what Intel has as its new default for linking 
in the openMP library.   Also, add linking of openMP library to osscollect, which needs it now and it is not found 
if the correct (intel or gnu) library is not provided.

Issue 8:
Identified and fixed cuda experiment issues on Tri-labs platforms.

Issue 9:
Update the cbtf-argonavis-gui tarball with new changes for accessing documentation.
Add new cuda gui tarball and update the support script version to 0.8.1.  

Issue 10:
Make papi-5.5.1 the default papi version.

Issue 11:
Tighten up the checks for finding boost, so we do not have to use force-build-build.  Look for the boost components that cbtf needs to be there.

Issue 12:
Add changes to allow the offline version of OSS to build on BGQ.  
Needed a means to use the compute node version of libmonitor.
Add an unwinding signal handler for segv.  Adding for Don - this aimed at the offline version of OSS.

Issue 13:
Update the name of the openMP specific collector in one of the code blocks that was missed in a previous update.  
Changed ompt to omptp.

Issue 14:
Fix an issue where the fact cray alps was found in the find package step but not recognized in the cbtf 
topology generation code because of the code needing a compiler definition (define) being added to 
the build of cbtf-core-mrnet and its static version. 

Issue 15:
Add find libmonitor cmake file for cbtf-argonavis.
Add code to check for libmonitor and the include and library files in the cuda collector build. 
Also, do not build cupti_avail if cupti version is less than 7.
Add the change in the build script to support the cbtf-argonavis libmonitor recognition.
Do not build the cuda collector when building on front-end for the Cray.  The collectors were built 
via the compute node build and there is no cbtf-krell/services built for the front-end, so compile errors occur.
Add passing of the target phrase to the cbtf-argonavis build.

Issue 16:
Updates to CLI views: Remove OpenMpi from the header text on the load balance view of a single rank that 
shows the load balance on the underlying OpenMP threads.  Only show ThreadId instead of OpenMP ThreadId.

Issue 17:
Remove Bfd_LIBRARY_SHARED reference from the daemonTool CMakeLists.txt file, it is/was not needed and 
was causing an issue in the spack build of cbtf-krell.  Also, take out unneeded reference to BFD shared 
library in the testXDR build cmake file.

Issue 18:
Fix issues discovered testing the python scripting api. The python interface is used via 
cbtf-offline.py.in when creating python scripts to run and view experiments with python 
and when doing same using the interactive cli commands (eg. expcreate,expattach,expgo). 
In the past the cbtf-offline.py.in managed the entire environment preparation and directly 
used the framework-cbtf instrumentor to launch mrnet etc. Now the cbtf-offline.py.in script 
will call the convenience script which handles the environment and osscollect which parses 
the command for mpi and openmp and then launches the mrnet network and experiment. 
Additionally, the pyscripting issue exposed an issue where the view code failed with 
unresolved symbols under python and where the openmp related view code was not actually 
creating omp work functions in the correct library (should be libopenss-cli based on how 
this is coded using .txx files).  Extensive testing now shows the python api is working 
and openmp speedups are active.

Issue 19:
Updates to the shared collection code in cbtf-krell pertaining to libmonitor callbacks and 
previous change that allowed cuda to track threads created during mpi start-up 
(which cbtf-krell collection previously ignored). That broke the newly added 
--offline mode since some mrnet specific callbacks need to exist for runtime 
symbols but should be a no-op (empty functions) for --offline mode. This change 
also updates libmonitor callbacks for fork handling based on some traces from failed 
runs of the cuda experiment on PFE.

Issue 20:
The tool needs to prefer openmp thread id's over posix thread id's. This fixes an 
additional case where posix thread id was tested BEFORE the openmp thread id, resulting 
in posix thread id's being displayed.  The tracing collectors provide a dm_id detail to 
allow displaying a rank/pid, threadid, or rank/pid:threadid pair in the -v trace output.  
This change enforces usage of the simple integer thread id (via monitor_thread_num used 
as the omp_tid in the collectors) as the thread identifier rather than the less useful 
(eg -t range) posix thread ids.  

Issue 21:
Created an unwinding signal handler for segv aimed at the offline version of OSS.

Issue 22:
Fix a problem building the semi-deprecated offline build of OSS.  We were including 
libopenss-framework-cbtf when building libopenss-cli for all versions.  This mod 
excludes that library when building the offline version of OSS.

Issue 23:
Fix compile time errors in cbtf-argonavis, where the boost:: qualifier was needed on 
four references to uint64_t.  Consultation and fixes provided by Bill Hachfeld.

Issue 24:
Update the OpenSpeedShop convenience script man pages and add ossmpip man page.

Issue 25:
-Wall compilation revealed a number of issues with missing function prototypes and 
format issues in various fprintf or sprintf calls.  In addition some function had 
a missing return or used a variable that was potentially uninitialized.  This fixes 
most of the issues.  For some of the missing prototypes, a FIXME has been addded to
note that proper include files need to be created to provide the missing extern definitions.

Issue 26:
Fix bug where the cbtf topogology generator, on a Cray, did not compute the correct number of application nodes needed in the case where the user requested 46 ranks and to run 23 ranks on each of two nodes.

Issue 27:
When checking for MPI Implementations, first look for the CBTF_MPI_IMPLEMENTATION variable, if that is not set look at OPENSS_MPI_IMPLEMENTATION (lots of module files out in the field are setting this variable), if neither are set, then use the default_mpi variable as the mpi implementation type (one of openmpi, mpich, mvapich, mvapich2, mpt...).

Issue 28:
Add MPI asynchronous non-blocking MPI-3 standard function wrappers to cbtf-krell.  Use/see the mpi_category==async_nonblocking for just gathering performance information for just the new asynchronous non-blocking MPI-3 standard functions.  These have been tested on a number of platforms including rzmerl, a DOD cray, x8664 clusters, and desktops using the osu-micro-benchmark suite.  This work was largely done by Patrick R.

Issue 29:
Add the MPI asynchronous non-blocking MPI-3 standard functions to the ossdrivers list of MPI functions. Also, add the mpi category list of functions (mpi_async_nonblocking_traceable_functions).

Issue 30:
Fix issue with parsing srun -n options specified without a space.
Tested by jeg.  Fix also verified not to break openmpi -n or -np
processing (openmpi allows -n as alias for -np but does not allow
a value without the intervening space).  Also removed cmake test
for boost unit_test_framework in cmake configure.

Issue 31:
Add a segv handler for SIGSEGV (and SIGBUS) signals raised due to
a failure in libunwind or our libunwind code.  If a segv is raised
while unwinding, we are left with a short stack but usertime does
not crash the users application.  Otherwise, segv's in the rest of
the code will behave as normal.  This code may need to be added to
the hwctime collector as well. I believe we added it to the old
offline usertime collector in openspeedshop to address this issue
for BGQ users.


Issue n: 
<Please report issues so that the issue to: oss-contact@krellinst.org so that it can be resolved>

-------------------------------------------------------------------------------------------------
NOTE:  Module files need to change for the CBTF based version:  HERE is and example for a cluster:
-------------------------------------------------------------------------------------------------

#%Module1.0#####################################################################
##
## openss modulefile
##
proc ModulesHelp { } {
        global version openss

        puts stderr "\topenss - loads the OpenSpeedShop software & application environment"
        puts stderr "\n\tThis adds $oss/* to several of the"
        puts stderr "\tenvironment variables."
        puts stderr "\n\tVersion $version\n"
}

#  NOTE -------------------------------------------------------------
#  The paths may need adjustment for different library naming schemes
#  NOTE -------------------------------------------------------------
#

module-whatis   "Loads the OpenSpeedShop runtime environment."

# for Tcl script use only
set     version         2.3

# Set up variables to reference later for the krell root, cbtf, and OpenSpeedShop proper
set     root            /opt/OSS/krellroot_v2.3.1.beta1
set     cbtf            /opt/OSS/cbtf_v2.3.1.beta1
set     cbtfk           /opt/OSS/cbtf_v2.3.1.beta1
set     oss             /opt/OSS/osscbtf_v2.3.1.beta1

#  XPLAT_RSH is needed for MRNet which is now needed for use in CBTF
setenv		XPLAT_RSH	ssh

#  For the mpi experiments only - specify the MPI implementation of your
#  application that will be run with OpenSpeedShop.   These are the
#  mpi, mpit, and mpip experiments.  All other experiment types will
#  ignore this setting.  It is only needed for mpi, mpit, and mpip.
setenv          CBTF_MPI_IMPLEMENTATION    openmpi
setenv          OPENSS_MPI_IMPLEMENTATION    openmpi

#  This is needed if you use the --offline argument following the
#  convenience scripts, for example:   osspcsamp --offline "mpirun -np 4 ./nbody"
#  This is the offline mode of operation which is now built into the
#  CBTF based version of OpenSpeedShop
setenv          OPENSS_RAWDATA_DIR    /opt/shared

# Only need these CBTF specific variables for situations where the environment is not passed
setenv          MRNET_COMM_PATH $cbtfk/sbin/cbtf_mrnet_commnode
setenv          CBTF_MRNET_BACKEND_PATH $cbtfk/sbin/cbtf_libcbtf_mrnet_backend

# Set up the paths for the OSS/CBTF version of OpenSpeedShop
prepend-path    PATH			$root/bin
prepend-path    PATH			$cbtf/bin
prepend-path    PATH			$cbtfk/sbin
prepend-path    PATH			$cbtfk/bin
prepend-path    PATH			$oss/bin
prepend-path    MANPATH			$oss/share/man

# Set up the dyninst runtime library path for the OSS/CBTF version of OpenSpeedShop
# This is required for finding loops and gathering symbol table information.
setenv DYNINSTAPI_RT_LIB $root/lib64/libdyninstAPI_RT.so

# Set up the library paths for the OSS/CBTF version of OpenSpeedShop
prepend-path LD_LIBRARY_PATH $root/lib64
prepend-path LD_LIBRARY_PATH $cbtf/lib64
prepend-path LD_LIBRARY_PATH $cbtfk/lib64
prepend-path LD_LIBRARY_PATH $oss/lib64

# Set up the python path so that the python scripting API can find
# the openss python module files.
setenv PYTHONPATH $oss/lib64/openspeedshop


